\documentclass[unicode,lefteqn,c,hyperref={pdfpagelabels=false}]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath,mathrsfs}
\usepackage[russian]{babel}
\usepackage{ulem}\normalem
\usepackage{color}
\usepackage[noend]{algorithmic}

\input macro.tex

\usetheme{Warsaw}
\usefonttheme[onlylarge]{structurebold}
\setbeamerfont*{frametitle}{size=\normalsize,series=\bfseries}
\setbeamertemplate{navigation symbols}{}
\setbeameroption{show notes}
\definecolor{beamer@blendedblue}{RGB}{15,80,120}
\let\Tiny=\tiny
\def\shortspace{\hspace{1.5pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[\hbox to 56mm{Проникающая радиация\hfill\insertframenumber\,/\,\inserttotalframenumber}]{Поражающие факторы ядерного взрыва: \\проникающая радиация}
\author[П.\shortspace ??.\shortspaceАндрианов, М.\shortspaceО.\shortspaceБурмистров]{П.\shortspace ??.\shortspaceАндрианов, М.\shortspaceО.\shortspaceБурмистров}
\institute{\vfill Московский физико-технический институт
		%\vfill Факультет управления и прикладной математики
		\vfill Военная кафедра}
\date{\today}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{frame}
    \titlepage
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Введение}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Данные и вероятностные гипотезы}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Данные и вероятностные гипотезы}
    \textbf{Исходные данные:}
    \begin{itemize}
    		\item $W$~--- конечное множество (словарь) слов;
    		\item $D$~--- конечное множество (коллекция) документов;
    		\item $n_{dw}$~--- число вхождений слова~$w\in W$ в~документ $d\in D$;
    \end{itemize}

    \smallskip
    \rlap{\textbf{Основные гипотезы вероятностной тематической модели:}}
    \begin{itemize}
    		\item $\exists T$ --- конечное множество латентных тем;
    		\item $D\times W\times T$ --- вероятностное пространство с~$p(d,w,t)$;
    		\item порядок слов в~документах не~важен;
    		\item $p(w|d,t) = p(w|t)$ --- гипотеза условной независимости;
    \end{itemize}

    \smallskip
    \textbf{Найти:}
    \begin{itemize}
    		\item $\theta_{td} = p(t|d)$ --- распределение тем в~каждом документе~$d$;
    		\item $\phi_{wt} = p(w|t)$ --- распределение слов в~каждой теме~$t$;
    \end{itemize}
\end{frame}

\begin{frame}{Вероятностная тематическая модель}
    Вероятностная модель порождения документа $d$:
    $$
        p(w|d)
        = \sum_{t\in T} p(w|t)\, p(t|d)
        = \sum_{t\in T} \phi_{wt} \theta_{td}
    $$

    \textbf{Метод PLSA:} максимизация правдоподобия
    $$
        p(D;\Theta,\Phi) = \prod_{d\in D} \prod_{w\in d} p(w|d)^{n_{dw}} \to \max\limits_{\Theta,\Phi};
    $$

    \textbf{Метод LDA:} дополнительно предполагается, что \\
    векторы $\theta_d$, $\phi_t$ порождаются распределением Дирихле:
    $$
        \theta_d \sim \textsf{Dir} (\alpha), \quad
        \phi_t \sim \textsf{Dir} (\beta). \quad
    $$ 
	Максимизация апостериорной вероятности.
\end{frame}

\subsection{Анализ решений}
\begin{frame}{Сравнение, модификации и обобщение методов}
    Для идентификации параметров $\Theta$, $\Phi$ используется \rlap{EM-алгоритм}.

	\textbf{Основные модификации:}
	\begin{itemize}
		\item PLSA-GEM позволяет ускорить сходимость за счет более частого обновления параметров;
		\item LDA-GS: одно вхождение слова в документ соответствует одной теме
	\end{itemize}
	
	\textbf{Цели работы:}
	\begin{itemize}
		\item обобщение известных алгоритмов тематического моделирования (PLSA-GEM и LDA-GS) в рамках одного численного метода;
		\item исследование зависимости качества построенной модели от параметров полученного метода.
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Алгоритм решения}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Обобщающий алгоритм}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Соединение PLSA-GEM и LDA-GS в общий численный метод}
\begin{algorithmic}[1]
    \REQUIRE
		$D$, $|T|$, приближения $\Theta$~и~$\Phi$, гиперпараметры $\alpha$, $\beta$;
    \ENSURE
        распределения $\Theta$~и~$\Phi$;
	\STATE $(t; h)_{dwi} = 0 $ для всех $d\in D, w \in W, \alert{i \in I_{dw}}$;
    \REPEAT
        \FORALL{$d\in D$,\; $w\in d$,\; $i\in I_{dw}$}
			\STATE подготовить тему $t_{dwi}$; \; $t$ = $t_{dwi}$;			
			\STATE вычислить значение $\delta$;
			\STATE увеличить $h_{dwi}$, $\varphi(w|t)$, $\theta(t|d)$ на $\delta$;		
			\IF{\alert{пора обновить параметры $\Phi$, $\Theta$}}
				\STATE обновить $\Theta$~и~$\Phi$;
			\ENDIF
        \ENDFOR
    \UNTIL $\Theta$ и~$\Phi$ не~стабилизируются.
\end{algorithmic} 
%Где $h_{dwi}$ --- сколько раз слово $w$ в $d$ было отнесено к теме $t_{dwi}.$
\end{frame}

\subsection{Варианты оптимизации параметров}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Изученные способы ускорения сходимости}
\textbf{Критерий обновления $\Theta$ и~$\Phi$:}
\begin{itemize}
		\item после каждого уникального слова в документе;
		\item через фиксированное несколько попыток \rlap{(в т.ч. постоянно);}
		\item после каждого документа;
		\item после всей коллекции.
\end{itemize}
\textbf{Число $|I_{dw}|$ тем, ассоциированных с уникальным $w$ в $d$:}
\begin{itemize}
		\item $|T|$;
		\item $\gamma \cdot n_{dw}$;
		\item $\min\cbr{n_{dw}, C}, \; C\text{ --- const}$.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Численный эксперимент}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Обучение --- контроль}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Способы контроля качества модели}
Функционал близости распределений: $$Q = \suml_{d\in D} \cfrac{n_d}n\;\rho\!\cbr{\cfrac{n_{dw}}{n_d}, \suml_{t\in T} \varphi(w|t)\theta(t|d)}$$
\begin{itemize}
		\item дивергенция Кульбака–Лейблера;
		% \item ненормированная $\chi^2$-статистика;
		% \item расстояние Хеллингера.
\end{itemize}
Обучение на 1000 документов. Контроль на 200 документах, случайно разбитых на 2 части: по одной с помощью обученной матрицы $\Phi$ оценивалось распределение слов в другой.
\end{frame}

\section*{}
\subsection{Заключение}
\begin{frame}{Заключение, результаты и выводы}
\begin{itemize}
		\item Построен численный метод, объединяющий существующие алгоритмы вероятностного тематического моделирования.
		\item Проведено исследование зависимости качества работы полученного алгоритма от параметров.
		%\item Показано преимущество LDA-GS перед PLSA-GEM.
		\item Показана возможность более редкого обновления параметров.
		\item Показано значительное ускорение сходимости алгоритма LDA-GS при уменьшении числа ассоциируемых тем.
\end{itemize}

\end{frame}

\end{document}
