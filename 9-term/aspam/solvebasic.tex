Итак, для нахождения значений $\mb a$ и $R$ необходимо решить следующую задачу
\begin{equation}
\label{min:main}
			\begcas{
			&R^2 + C\suml_i\xi_i \to \min\limits_{\mb a, R,\mb\xi}, \\
			&\norm{\mb x_i-\mb a}^2\le R^2 + \xi_i,\quad\xi_i\ge0,\quad i = 1,\ldots,N.
			}
\end{equation}
Функция Лагранжа этой задачи имеет вид
\begin{equation*}
	\Ell(\mb a, R,\mb\xi,\mb\alpha,\mb\gamma)=
		R^2 + C\suml_i\xi_i - \suml_i\gamma_i\xi_i
		- \suml_i\alpha_i\cbr{R^2 + \xi_i - \cbr{\mb x_i\T\mb x_i - 2\mb a\T\mb x_i + \mb a\T\mb a}},
\end{equation*}
где $\alpha_i\ge 0$  и $\gamma_i\ge 0$ --- множители Лагранжа.
Необходимым условием минимума является равенство нулю частных производных функции Лагранжа по всем переменным
\begin{equation}
	\label{minLag}
	\begin{array}{ll}
		\dd \Ell R = 0: & \suml_i \alpha_i = 1 \:\cbr{\text{случай $R = 0$ рассмотрим отдельно},}\\
		\dd \Ell{\mb a} = 0: & \mb a = \cfrac{\suml_i\alpha_i\mb x_i}{\suml_i \alpha_i}= \suml_i \alpha_i\mb x_i,\\
		\dd \Ell{\xi_i} = 0: & \gamma_i = C - \alpha_i, \;\; i = 1,\ldots,N.
	\end{array}
\end{equation}

Из последнего уравнения получаем, что $\alpha_i = C - \gamma_i$.
Таким образом, мы получаем новые ограничения на $\alpha_i$
$$0\le\alpha_i\le C, \;\; i = 1,\ldots,N.$$

Если это ограничение выполнено, то мы можем вычислить $\gamma_i$ по формуле $\gamma_i = C - \alpha_i$, и при этом автоматически будет выполнено условие $\gamma_i\ge 0$.

Тогда для функции Лагранжа получим выражение
{
\newcommand\ai[0]{\alpha_i\,}
\newcommand\aj[0]{\alpha_j\,}
\newcommand\mbx[1]{\mb x_#1}
%\newcommand\cd[0]{\!\cdot\!}
\newcommand\cd[0]{\T}
\al{
\Ell(\mb a, R,&\mb\xi,\mb\alpha,\mb\gamma)
	= 	R^2 - \suml_i\ai R^2 + C\suml_i\xi_i - \suml_i\ai\xi_i  +\\
	&+ \suml_i\ai\mbx i\cd\mbx i -  2\suml_i\ai\mb a\cd\mbx i + \suml_i\ai\mb a\cd\mb a - \suml_i\gamma_i\xi_i =\\
	&= R\cd R\cd\cbr{1-\suml_i\ai} +\suml_i\xi_i\cbr{C-\ai-\gamma_i} + \\
	&+ 	\suml_i\ai\mbx i\cd\mbx i - 2 \suml_i\ai\suml_j\aj\mbx j\cd\mbx i
	+ 	\suml_{i,j}\ai\aj \mbx j\cd \mbx i = \\
	&= 	\suml_i\ai\mbx i\cd\mbx i-\suml_{i,j}\ai\aj\mbx j\cd\mbx i \to \max_{\mb\alpha}.
}
}

Полученное выражение является квадратичной формой.
Тогда его максимум находится по известным алгоритмам решения задач квадратичного программирования.
По оптимальным значениям $\mb{\alpha}$ мы сможем найти оптимальное значение центра гипершара $\mb a$ и отступов $\mb{\xi}$, используя соотношения (\ref{minLag}).

Для каждого объекта $\mb x_i$ оптимальное значение $\alpha_i$ (или же $\gamma_i = C-\alpha_i$) задает тип принадлежности объекта построенному гипершару:
\begin{itemize}
	\item $\alpha_i=0 \Rightarrow \text{объект $\mb x_i$ лежит внутри гипершара, имеет нулевой отступ};$
	\item $0<\alpha_i<C \Rightarrow \text{объект $\mb x_i$ лежит на границе гипершара, имеет нулевой отступ};$
	\item $\alpha_i=C \Rightarrow \text{объект $\mb x_i$ лежит вне гипершара, имеет ненулевой отступ}.$
\end{itemize}

%Те векторы $\mb x_i$, для которых $\alpha_i=0$ и $\gamma_i=C$, лежат внутри гиперсферы, те, для которых $0<\alpha_i<C$ и $0<\gamma_i<C$~--- на её границе, а те, для которых $\alpha_i=C$ и $\gamma_i=0$, лежат вне гиперсферы и имеют ненулевой отступ $\mb\xi$.

Радиус $R$ определяется как расстояние от центра гипершара $\mb a$ до опорных векторов, лежащих на границе гипершара.

Если же $R = 0$, то задача (\ref{min:main}) имеет вид
\begin{equation}
	\begcas{
	&C\suml_i\xi_i \to \min\limits_{\mb a, \mb\xi}, \\
	&\norm{\mb x_i-\mb a}^2\le \xi_i,\quad\xi_i\ge0,\quad i = 1,\ldots,N.
	}
\end{equation}
т.е.
\begin{equation}
			C\suml_i\norm{\mb x_i-\mb a}^2 \to \min\limits_{\mb a},
\end{equation}
а эта задача соответствует методу наименьших квадратов. Тогда $\mb a = \frac{\sum_i \mb x_i}N$.
При этом следует понимать, что значение $R=0$ обнуляет обобщающую способность нашего классификатора, поэтому следует отказываться от такого решения, если есть выбор.
Здесь же стоит отметить, что $R = 0$ обязательно, если $C < \frac1N,$ где $N$~--- число объектов в обучающей выборке, поскольку в этом случае условия на $\mb \alpha$ несовместны.

Для возможности описания данных более гибкой формой, нежели сфера, в работе \cite{Tax2001} предлагается использовать потенциальные функции \cite{Izerman1979}. Наиболее часто используемыми потенциальными функциями являются полиномиальная
$$K_p(\mb x_i, \mb x_j) = \cbr{1 + \mb x_i\T \mb x_j}^p$$
и радиальная базисная функция Гаусса
$$K(\mb x_i, \mb x_j) = \exp\cbr{-\frac{\norm{\mb x_i - \mb x_j}^2}{2s^2}}.$$
Таким образом, чтобы получить улучшенную модель описания данных, необходимо заменить в функции Лагранжа операцию вычисления
скалярного произведения двух векторов вычислением значения потенциальной функции двух аргументов.

При таком обобщении решающее решающее правило (\ref{rule:basic}) принимает вид 
\begin{align}
	\label{rule:kernel}
	f(\mb x) 
		&= 	\sbr{\norm{\mb x-\mb a} \le R} 
		= 	\sbr{\mb x\cdot\mb x - 2\mb x\cdot\mb a + \mb a\cdot\mb a \le R^2} = \notag \\
		&= 	\sbr{\mb x\cdot\mb x - 2\mb x\cdot\cbr{\suml_{i} \alpha_i \mb x_i} + \cbr{\suml_{i} \alpha_i \mb x_i}\cdot\cbr{\suml_{i} \alpha_i \mb x_i} \le R^2} \to \notag \\
		&\to \sbr{K(\mb x,\mb x) - 2\suml_{i} \alpha_i K(\mb x, \mb x_i) + \suml_{i,j} \alpha_i\alpha_jK(\mb x_i,\mb x_j) \le R^2}.
\end{align}

Здесь оптимальное значение $R$ определяется как значение выражения $$K(\mb x,\mb x) - 2\suml_{i} \alpha_i K(\mb x, \mb x_i) + \suml_{i,j} \alpha_i\alpha_jK(\mb x_i,\mb x_j)$$ для граничных объектов ($0<\alpha_i<C$).
