Объектом исследования является множество электронных сообщений характеризуемых некоторым набором признаков.
Рассмотрим одноклассовую классификацию объектов генеральной совокупности $\Omega$.
Пусть каждый объект $\omega \in{\Omega}$  представлен точкой в линейном пространстве признаков
$\mb x(\omega)=\cbr{x^1(\omega),\ldots, x^n(\omega)} \in {\mathbb R^n}$. При этом мы изучаем лишь объекты одного класса, поэтому меткой класса объект существенно не обладает. 
Тем не менее нашей задачей будет построение классификатора, который будет давать ответ $1$, если предъявленный объект лежит в множестве, и $0$ иначе.

В работе \cite{} предлагается строить сферический пороговый классификатор вида 
$\sbr{z \le 0} $, где $z(\mb x,\mb a,R)=\norm{\mb x-\mb a}-R$ без вероятностного обоснования такого подхода. При этом в области $z(\mb x, \mb a, R) \ge 0$ значение этой величины несёт смысл отступа $\delta$.
%Множество $\Omega^*$ объектов $x_j, j=1 \ldots N,$ на которых известна скрытая характеристика $y_j$, назовем обучающей совокупностью. 
%В обучающей совокупности будут только объекты класса, то есть $y_j=1$ для всех $j=1\ldots N$.

\addtikz{basic}{1.2}{thick,domain=-1:2}{basic picture}

Будем придерживаться вероятностной модели распределения объектов генеральной совокупности.
Параметрическое семейство условных плотностей распределения в признаковом пространстве имеет вид 
\begin{equation}
	\label{PhiXARC}
	\varphi \cbr{ {\mb x | \mb a,R;c} } =
		\begcas{
			&1 				\qquad  	\: 	z(\mb x,\mb a,R) \le 0, \\
			&e^{-c z(\mb x,\mb a,R)}, 	\:	z(\mb x,\mb a,R) \ge 0.
		} 
\end{equation}
Здесь величина $c$ является гиперпараметром.

\addtikz{expon}{1.2}{thick,domain=0:4}{Значение плотности распределения вдоль радиуса}

Совместную плотность распределения случайной обучающей совокупности будем понимать как плотность распределения выборки независимых реализаций
$$\Phi(\mb X|\mb a,R)=\prod_{j=1}^N \varphi(\mb x_j|\mb a,R),$$ 
где $\mb X=\fbr{\mb x}_{j=1}^N$.
Пусть, далее, выбрана априорная плотность совместного распределения вероятностей $\Psi(\mb a,R)$ для параметров распределения $\varphi \cbr{\mb x | \mb a,R;c}$. 
Тогда апостериорная плотность распределения параметров $\mb a$ и $R$ относительно обучающей совокупности определяется формулой Байеса
\begin{equation}
	\label{Prob:aRX}
	p(\mb a,R|\mb X)
	= \frac {\Psi(\mb a,R) \Phi(\mb X|\mb a,R)}
			{\int {\Psi(\mb a',R') \Phi(\mb X|\mb a',R')d\mb a'dR'}}.
\end{equation}

Из принципа максимума плотности апостериорного распределения в пространстве параметров модели генеральной совокупности получим байесовское правило обучения:
\begin{equation}
	\label{argmax:full}
	\cbr{\hat{\mb a},\hat R|\mb X} = \argmax{\mb a,R} p(\mb a,R|\mb X)
\end{equation}

Поскольку знаменатель в выражении (\ref{Prob:aRX}) не зависит от целевых переменных: 
$$p(\mb a,R|\mb X) 
	\propto \Psi(\mb a,R) \Phi(\mb X|\mb a,R) 
	=  \Psi(\mb a,R) \prod_{j=1}^N \varphi(\mb x_j|\mb a,R),$$
то в задаче максимизации (\ref{argmax:full}) достаточно рассматривать только числитель
$$\cbr{\hat{\mb a},\hat R|\mb X}
	= \argmax{\mb a,R} p(\mb a,R|\mb X) 
	= \argmax{\mb a,R} \cbr{\log\Psi(\mb a,R) + \sum_{j=1}^N\log\varphi(\mb x_j|\mb a,R)}. $$


