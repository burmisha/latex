Объектом исследования является множество электронных сообщений характеризуемых некоторым набором признаков.
Рассмотрим одноклассовую классификацию объектов генеральной совокупности $\Omega$.
Пусть каждый объект $\omega \in{\Omega}$  представлен точкой в линейном пространстве признаков
$\mb x(\omega)=\cbr{x^1(\omega),\ldots, x^n(\omega)} \in {\mathbb R^n}$. При этом мы изучаем лишь объекты одного класса, поэтому меткой класса объект существенно не обладает.
Тем не менее нашей задачей будет построение классификатора, который будет давать ответ $1$, если предъявленный объект лежит в множестве, и $0$ иначе.

В работе \cite{Tax2001}  предлагается строить сферический пороговый классификатор вида
$\sbr{z \le 0} $, где $z(\mb x,\mb a,R)=\norm{\mb x-\mb a}-R$ без вероятностного обоснования такого подхода. При этом в области $z(\mb x, \mb a, R) \ge 0$ значение величины $\norm{\mb x-\mb a}^2-R^2$ несёт смысл отступа $\xi$, а для объектов внутри шара отступ полагается равным 0. Для подбора значений $\mb a, R$ решается задача
\begin{equation}
	\label{min:noprob}
	F(R,\mb a, \mb \xi) = R^2 + C\sum_i\xi_i \to \min_{\mb a, R, \mb\xi},
\end{equation}
при этом здесь и далее мы полагаем, что суммирование по индексу $i$ (а в дальнейшем и $j$) означает суммирование по всем объектам обучающей выборки.

Здесь величина $C$ задает баланс между минимальным объёмом шара и наименьшим числом объектов обучающей выборки вне сферы. Пример описания объектов шаром приведен на рисунке \ref{tikz:basic}.

\addtikz{basic}{1.2}{thick,domain=-1:2}{Пример описания объектов шаром}

Будем придерживаться вероятностной модели распределения объектов генеральной совокупности.
Параметрическое семейство условных плотностей распределения в признаковом пространстве имеет вид
\begin{equation}
	\label{PhiXARC}
	\varphi \cbr{ {\mb x | \mb a,R;c} } \propto
		\begcas{
			&1, 		\qquad\qquad\qquad  	\! 	z(\mb x,\mb a,R) < 0, \\
			&e^{-c \cbr{\norm{\mb x-\mb a}^2-R^2}}, 	\:	z(\mb x,\mb a,R) \ge 0.
		}
\end{equation}
Здесь величина $c$ является гиперпараметром. График данной функции плотности изображен на рисунке \ref{tikz:expon}.

\addtikz{expon}{1.2}{thick,domain=0:4}{Значение плотности распределения вдоль радиуса}

Совместную плотность распределения случайной обучающей совокупности будем понимать как плотность распределения выборки независимых реализаций
$$\Phi(\mb X|\mb a,R)=\prod_{j=1}^N \varphi(\mb x_j|\mb a,R),$$
где $\mb X=\fbr{\mb x}_{j=1}^N$.
Пусть, далее, выбрана априорная плотность совместного распределения вероятностей $\Psi(\mb a,R)$ для параметров распределения $\varphi \cbr{\mb x | \mb a,R;c}$.
Тогда апостериорная плотность распределения параметров $\mb a$ и $R$ относительно обучающей совокупности определяется формулой Байеса
\begin{equation}
	\label{Prob:aRX}
	p(\mb a,R|\mb X)
	= \frac {\Psi(\mb a,R) \Phi(\mb X|\mb a,R)}
			{\int {\Psi(\mb a',R') \Phi(\mb X|\mb a',R')d\mb a'dR'}}.
\end{equation}

Из принципа максимума плотности апостериорного распределения в пространстве параметров модели генеральной совокупности получим байесовское правило обучения
\begin{equation}
	\label{argmax:full}
	\cbr{\hat{\mb a},\hat R|\mb X} = \argmax_{\mb a,R} p(\mb a,R|\mb X)
\end{equation}

Поскольку знаменатель в выражении (\ref{Prob:aRX}) не зависит от целевых переменных
$$p(\mb a,R|\mb X)
	\propto \Psi(\mb a,R) \Phi(\mb X|\mb a,R)
	=  \Psi(\mb a,R) \prod_{j=1}^N \varphi(\mb x_j|\mb a,R),$$
то в задаче максимизации (\ref{argmax:full}) достаточно рассматривать только числитель
$$\cbr{\hat{\mb a},\hat R|\mb X}
	= \argmax_{\mb a,R} p(\mb a,R|\mb X)
	= \argmax_{\mb a,R} \cbr{\ln\Psi(\mb a,R) + \sum_{j=1}^N\ln\varphi(\mb x_j|\mb a,R)}. $$

Теперь покажем, что задача в такой постановке обобщает задачу (\ref{min:noprob}).
Положим, что априорное распределение параметров $\Psi(\mb a,R)$ обладает следующими свойствами:
\begin{itemize}
 	\item $\mb a$ и $R$~--- случайные независимые величины,
 	\item $\modul{R}$ --- нормально распределенная случайная величина с нулевым математическим ожиданием и дисперсией $\sigma^2$,
 	\item $\mb a$ равномерно распределено по всему пространству $\Rn$ (такое распределение будет несобственным \cite{Groot1974}).
 \end{itemize}
 Тогда совместное распределение параметров также будет несобственным
 $$\Psi(\mb a,R)\propto e^{-\frac1{2\sigma^2}R^2}.$$
 Подставим это выражение и функцию распределения из (\ref{PhiXARC})
 \begin{align}
 	\label{ProbARX_simple}
 	\ln p(\mb a,R&|\mb X)
 		=	\ln\Psi(\mb a,R) + \sum_{j=1}^N\ln\varphi(\mb x_j|\mb a,R) =\notag \\
 		&= 	-\frac{R^2}{2\sigma^2} + \sum_{i: \norm{\mb x_i - \mb a} \le R} \ln 1
 			+ \sum_{i: \norm{\mb x_i - \mb a} > R} \ln e^{-c \cbr{\norm{\mb x-\mb a}^2-R^2}}  = \notag \\
 		&= -\frac{R^2}{2\sigma^2} - \sum_{i: \norm{\mb x_i - \mb a} > R} c\cbr{\norm{\mb x_i - \mb a}^2 - R^2} = \notag \\
 		&= -\frac1{2\sigma^2}\cbr{R^2 + 2\sigma^2 c\sum_{i: \norm{\mb x_i - \mb a} > R} \cbr{\norm{\mb x_i - \mb a}^2 -R^2}} \to \max_{\mb a, R}.
 \end{align}
 Очевидно, задачи (\ref{ProbARX_simple}) и (\ref{min:noprob}) эквивалентны при $C = 2\sigma^2 c.$
