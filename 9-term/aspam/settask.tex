Объектом исследования является множество электронных сообщений характеризуемых некоторым набором признаков.
Рассмотрим одноклассовую классификацию объектов генеральной совокупности $\Omega$.
Пусть каждый объект $\omega \in{\Omega}$  представлен точкой в линейном пространстве признаков
$\mb x(\omega)=\cbr{x^1(\omega),\ldots, x^n(\omega)} \in {\mathbb R^n}$. При этом мы изучаем лишь объекты одного класса, поэтому меткой класса объект существенно не обладает. 
В работе \cite{} предлагается строить сферический пороговый классификатор
$z(\mb x,\mb a,R)=\sbr{\norm{\mb x-\mb a}-R}$ без вероятностного обоснования такого подхода.
%Множество $\Omega^*$ объектов $x_j, j=1 \ldots N,$ на которых известна скрытая характеристика $y_j$, назовем обучающей совокупностью. 
%В обучающей совокупности будут только объекты класса, то есть $y_j=1$ для всех $j=1\ldots N$.

\addepssize{fig1}{}{130}

Будем придерживаться вероятностной модели распределения объектов генеральной совокупности.
Параметрическое семейство условных плотностей распределения в признаковом пространстве имеет вид 
\begin{equation}
	\label{prob1}
	\varphi \cbr{ {\mb x | \mb a,R;c} } =
		\begcas{
			&e^{-c} \equiv \text{const}, \: 	z(\mb x,\mb a,R) \ge 0, \\
			&e^{-c z(\mb x,\mb a,R)}, \quad\:	z(\mb x,\mb a,R) \le 0.
		} 
\end{equation}
Здесь величина $c$ является гиперпараметром.

\addepssize{fig2}{Значение плотности распределения вдоль радиуса}{100} 
% TODO: 1) y in phi 	2) \norm{x-a} on X axis, not z

Совместную плотность распределения случайной обучающей совокупности будем понимать как плотность распределения выборки независимых реализаций
$$\Phi(\mb X|\mb a,R)=\prod_{j=1}^N \varphi(\mb x_j|\mb a,R),$$ 
где $\mb X=\fbr{\mb x}_{j=1}^N$.
Пусть, далее, выбрана априорная плотность совместного распределения вероятностей $\Psi(\mb a,R)$ для параметров распределения $\varphi \cbr{\mb x | \mb a,R;c}$. 
Тогда апостериорная плотность распределения параметров $\mb a$ и $R$ относительно обучающей совокупности определяется формулой Байеса
$$p(\mb a,R|\mb X)
	= \frac {\Psi(\mb a,R) \Phi(\mb X|\mb a,R)}
			{\int {\Psi(\mb a',R') \Phi(\mb X|\mb a',R')d\mb a'dR'}}.$$

Поскольку знаменатель не зависит от целевых переменных, то достаточно рассматривать только числитель
$$p(\mb a,R|\mb X) \propto \Psi(\mb a,R) \Phi(\mb X|\mb a,R) = 
\Psi(\mb a,R) \prod_{j=1}^N \varphi(\mb x_j|\mb a,R).$$

Из принципа максимума плотности апостериорного распределения в пространстве параметров модели генеральной совокупности получим байесовское правило обучения:
$$\cbr{\hat{\mb a},\hat R|\mb X}
	= \argmax{\mb a,R} p(\mb a,R|\mb X) 
	= \argmax{\mb a,R} \log\Psi(\mb a,R) + \sum_{j=1}^N\log\varphi(\mb x_j|\mb a,R). $$




