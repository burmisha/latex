\documentclass[12pt,a4paper]{amsart}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}
\usepackage{color}
\usepackage{epstopdf} % http://tex.stackexchange.com/questions/29664/latex-error-unknown-graphics-extension-eps
\usepackage[colorlinks,unicode,pdfpagelabels=true]{hyperref}
\usepackage[english,russian]{babel}
\usepackage{amssymb,enumerate}
\usepackage{enumitem}  \setlist{nolistsep}
\usepackage{tikz}
% \usepackage{textcase} % for UTF-8 in header

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% 	\input macro.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\newcommand\ol[1]{\overline{#1}}

	\def\cond{\,|\,}
	\newcommand\normal[2]{\mathcal{N}\!\cbr{#1,#2}}

	\newcommand\al[1]{\begin{align*} #1 \end{align*}}
	\newcommand\begcas[1]{\begin{cases}#1\end{cases}}

	\def\le{\leqslant}
	\def\ge{\geqslant}
	\def\Ell{\mathcal{L}}
	\def\Rn{\ensuremath{\mathbb{R}^n}}

	\newcommand\mb[1]{\ensuremath{\boldsymbol{\mathbf{#1}}}}
	\newcommand\argmax[1]{\arg\underset{#1}\max\,} % \operatornamewithlimits
	\newcommand{\prodl}{\prod\limits}
	\newcommand{\suml}{\sum\limits}

	\newcommand\cbr[1]{\left(#1\right)} %circled brackets
	\newcommand\fbr[1]{\left\{#1\right\}} %figure brackets
	\newcommand\sbr[1]{\left[#1\right]} %square brackets
	\newcommand\modul[1]{\left|#1\right|}
	\newcommand\norm[1]{\ensuremath{\left\|{#1}\right\|}}

	\newcommand{\T}{^{\text{\tiny\sffamily\upshape\mdseries T}}}
	\newcommand\dd[2]{\frac{\partial#1}{\partial#2}}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%% 			\input{abstract.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\title{Вероятностная модель одноклассовой классификации}
	\author{М.\,О.\,Бурмистров, Л.\,Н.\,Сандуляну}
	\address{Московский физико-технический институт, ФУПМ, каф. <<Интеллектуальные системы>>}
	% \thanks{Научный руководитель О.\,В.\,Красоткина}
	\date{декабрь 2012\,г.}

	\begin{abstract}
	Решается задача одноклассовой классификации электронных писем на предмет наличия в них спама. 
	В работе вводится квазивероятностная модель для классической эмпирической постановки задачи одноклассовой классификации и 
	производится сведение классического подхода к новой модели.
	%Произведена модификация, позволяющая накладывать различные требования отбора признаков. 
	Построенные методы классификации проверяются вычислительными экспериментами на модельных и реальных данных.
	\end{abstract}

\maketitle
\section{Введение} 							
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%		\input{intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	С широким развитием сети интернет и её проникновением в большую часть всех сфер жизни, у людей появилась возможность свободно обмениваться информацией и получать доступ к разнообразным ресурсом. 
	Одним из наиболее распространенных способов общения людей через интернет является использование электронной почты. 
	В силу большой открытости этого канала связи с точки зрения возможности передачи любого сообщения произвольному пользователю он активно используется мошенниками, злоумышленниками и распространителями рекламных материалов. При этом создается не только повышенная нагрузка на техническую инфраструктуру, но и тратится время людей, которым приходится отделять полезную информацию, от всей остальной. 
	Поэтому задача автоматизации фильтрации электронной почты будет оставаться актуальной в течение всего времени её существования.

	Задача фильтрации спама уже решалась многими методами \cite{Islam2007, Sun2008}, однако они в большой степени являлись эвристическими и не имели под собой четкой вероятностной модели. 
	Также проблемой является корректное составление обучающей выборки. 
	Дело в том, что спам-письма зачастую шаблонны и имеют много общего в своей структуре, к тому же они широко доступны. 
	Составить же обучающую выборку, содержащую письма, полезные для пользователей, гораздо сложнее по следующим причинам:
	\begin{itemize}
		\item меньшая доступность,
		\item высокая разнородность,
		\item большое число шаблонных писем (разнообразные уведомления от сервисов).
	\end{itemize}
	По этим причинам предлагается использовать методы одноклассовой классификации \cite{Tax2001, Khan2006}, чтобы отказаться от требования к обучающей выборки содержать достаточно широкое множество разнообразных представителей обоих классов.

	В работе предложена квазивероятностная постановка задачи одноклассовой классификации. 
	За счет такого подхода становятся яснее области применимости построенной модели и предъявляемые требования к данным.
	% Поскольку количество признаков, которые можно извлечь из текстов спам-писем, очень велико, то предлагается применить отбор признаков. 
	На основе полученной вероятностной постановки задачи, строится новая вероятностная модель порождения объектов, в ходе оптимизации которой происходит построение классификатора. %требуемый отбор признаков.

	Полученные методы построения одноклассовых классификаторов применяются к модельным и реальным данным.


\section{Байесовская постановка задачи}  	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%		\input{settask}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Объектом исследования является множество электронных сообщений характеризуемых некоторым набором признаков.
	Рассмотрим одноклассовую классификацию объектов генеральной совокупности $\Omega$.
	Пусть каждый объект $\omega \in{\Omega}$  представлен точкой в линейном пространстве признаков
	$\mb x(\omega)=\cbr{x^1(\omega),\ldots, x^n(\omega)} \in {\mathbb R^n}$. При этом мы изучаем лишь объекты одного класса, поэтому меткой класса объект существенно не обладает. 
	Тем не менее нашей задачей будет построение классификатора, который будет давать ответ $1$, если предъявленный объект лежит в множестве, и $0$ иначе.

	В работе \cite{Tax2001}  предлагается строить сферический пороговый классификатор вида 
	$\sbr{z \le 0} $, где $z(\mb x,\mb a,R)=\norm{\mb x-\mb a}-R$ без вероятностного обоснования такого подхода. При этом в области $z(\mb x, \mb a, R) \ge 0$ значение величины $\norm{\mb x-\mb a}^2-R^2$ несёт смысл отступа $\xi$, а для объектов внутри шара отступ полагается равным 0. Для подбора значений $\mb a, R$ решается задача
	\begin{equation}
		\label{min:noprob}
		F(R,\mb a, \mb \xi) = R^2 + C\sum_i\xi_i \to \min_{\mb a, R, \mb\xi}.
	\end{equation}
	Здесь величина $C$ задает баланс между минимальным объёмом шара и наименьшим числом объектов обучающей выборки вне сферы. Пример описания объектов щаром приведен на рисунке \ref{eps:example}.

	%\addtikz{basic}{1.2}{thick,domain=-1:2}{Пример описания объектов шаром}
	\begin{figure} [!ht] %lrp
		\centering
		\begin{tikzpicture}[x=1.2cm,y=1.2cm,thick,domain=-1:2]
			\newcommand\cird[1]{\draw [fill = lightgray] (center) ++ (#1) circle (0.04);} %, thin]

			\node (center) at (0.7,1.2) {};
			\draw[-latex] (-0.7,0) -- (4,0) node[below] {$x_1$};
			\draw[-latex] (0,-0.5) -- (0,3) node[left] {$x_2$};
			\draw [fill = none] (center) circle (1.0);
			\draw [fill = black] (center) circle (0.02);
			\cird{0.2,0.3}
			\cird{-0.13,0.6}
			\cird{0.17,-0.1}
			\cird{0.6,-0.2}
			\cird{-0.4,-0.5}
			\cird{-0.6,0.05}
			\cird{0.23,-0.85}
			\cird{-0.35,0.85}
			\cird{-0.8,0.4}
			\cird{0.75,0.6}

			\draw[fill = lightgray] (center) ++(75:1) ++(-15:2.2)circle (0.04) node[above]{$\mb x_i$};
			\draw[latex-latex, thin] (center) ++(75:1) -- ++(-15:2.2) node[pos=0.5,above]{$\sqrt\xi_i$};

			\draw[fill = lightgray] (center) ++(205:1) ++(115:1.3)circle (0.04) node[above]{$\mb x_j$};
			\draw[latex-latex, thin] (center) ++(205:1) -- ++(115:1.3) node[pos=0.5,left]{$\sqrt\xi_j$};

			\draw[latex-] (center) ++(240:1) -- +(60:1) node[pos=0.25, right] {$R$};
			\node[left] at (center) {$a$};
		\end{tikzpicture}
		\vspace{-10pt}
		\caption{Пример описания объектов шаром}
		\label{tikz:basic}	
	\end{figure}

	Будем придерживаться вероятностной модели распределения объектов генеральной совокупности.
	Параметрическое семейство условных плотностей распределения в признаковом пространстве имеет вид 
	\begin{equation}
		\label{PhiXARC}
		\varphi \cbr{ {\mb x | \mb a,R;c} } \propto
			\begcas{
				&1, 		\qquad\qquad\qquad  	\! 	z(\mb x,\mb a,R) < 0, \\
				&e^{-c \cbr{\norm{\mb x-\mb a}^2-R^2}}, 	\:	z(\mb x,\mb a,R) \ge 0.
			} 
	\end{equation}
	Здесь величина $c$ является гиперпараметром. График данной функции плотности изображен на рисунке \ref{tikz:expon}.

	\begin{figure} [!ht] %lrp
		\centering
		\begin{tikzpicture}[x=1.2cm,y=1.2cm,thick,domain=0:4]
			\draw[-latex] (-0.5,0) -- (5,0) node[below] {$\|\mb x-\mb a\|$};
			\draw[-latex] (0,-0.3) -- (0,1.8) node[left] {$\varphi(\mb x | \mb a, R)$};

			\draw[domain=0:1.2] plot (\x,1);
			\draw[domain=1.2:4.5] plot (\x,{exp(-0.2*(\x*\x-1.44))});
			\draw[dashed] (1.2,0) -- ++(0,1) ++(0,-1) node[below] {$R$};

			\node[left] at (0,1) {$1$};
			\node[below left] at (0,0) {$0$};
		\end{tikzpicture}
		\vspace{-10pt}
		\caption{Значение плотности распределения вдоль радиуса}
		\label{tikz:expon}	
	\end{figure}

	Совместную плотность распределения случайной обучающей совокупности будем понимать как плотность распределения выборки независимых реализаций
	$$\Phi(\mb X|\mb a,R)=\prod_{j=1}^N \varphi(\mb x_j|\mb a,R),$$ 
	где $\mb X=\fbr{\mb x}_{j=1}^N$.
	Пусть, далее, выбрана априорная плотность совместного распределения вероятностей $\Psi(\mb a,R)$ для параметров распределения $\varphi \cbr{\mb x | \mb a,R;c}$. 
	Тогда апостериорная плотность распределения параметров $\mb a$ и $R$ относительно обучающей совокупности определяется формулой Байеса
	\begin{equation}
		\label{Prob:aRX}
		p(\mb a,R|\mb X)
		= \frac {\Psi(\mb a,R) \Phi(\mb X|\mb a,R)}
				{\int {\Psi(\mb a',R') \Phi(\mb X|\mb a',R')d\mb a'dR'}}.
	\end{equation}

	Из принципа максимума плотности апостериорного распределения в пространстве параметров модели генеральной совокупности получим байесовское правило обучения
	\begin{equation}
		\label{argmax:full}
		\cbr{\hat{\mb a},\hat R|\mb X} = \argmax{\mb a,R} p(\mb a,R|\mb X)
	\end{equation}

	Поскольку знаменатель в выражении (\ref{Prob:aRX}) не зависит от целевых переменных 
	$$p(\mb a,R|\mb X) 
		\propto \Psi(\mb a,R) \Phi(\mb X|\mb a,R) 
		=  \Psi(\mb a,R) \prod_{j=1}^N \varphi(\mb x_j|\mb a,R),$$
	то в задаче максимизации (\ref{argmax:full}) достаточно рассматривать только числитель
	$$\cbr{\hat{\mb a},\hat R|\mb X}
		= \argmax{\mb a,R} p(\mb a,R|\mb X) 
		= \argmax{\mb a,R} \cbr{\ln\Psi(\mb a,R) + \sum_{j=1}^N\ln\varphi(\mb x_j|\mb a,R)}. $$

	Теперь покажем, что задача в такой постановке обобщает задачу (\ref{min:noprob}). 
	Положим, что априорное распределение параметров $\Psi(\mb a,R)$ обладает следующими свойствами:
	\begin{itemize}
	 	\item $\mb a$ и $R$~--- случайные независимые величины,
	 	\item $\modul{R}$ --- нормально распределенная случайная величина с нулевым математическим ожиданием и дисперсией $\sigma^2$,
	 	\item $\mb a$ равномерно распределено по всему пространству $\Rn$ (такое распределение будет несобственным \cite{Groot1974}).
	 \end{itemize} 
	 Тогда совместное распределение параметров также будет несобственным 
	 $$\Psi(\mb a,R)\propto e^{-\frac1{2\sigma^2}R^2}.$$
	 Подставим это выражение и функцию распределения из (\ref{PhiXARC})
	 \begin{align}
	 	\label{ProbARX_simple}
	 	\ln p(\mb a,R&|\mb X) 
	 		=	\ln\Psi(\mb a,R) + \sum_{j=1}^N\ln\varphi(\mb x_j|\mb a,R) =\notag \\
	 		&= 	-\frac{R^2}{2\sigma^2} + \sum_{i: \norm{\mb x_i - \mb a} \le R} \ln 1 
	 			+ \sum_{i: \norm{\mb x_i - \mb a} > R} \ln e^{-c \cbr{\norm{\mb x-\mb a}^2-R^2}}  = \notag \\
	 		&= -\frac{R^2}{2\sigma^2} - \sum_{i: \norm{\mb x_i - \mb a} > R} c\cbr{\norm{\mb x_i - \mb a}^2 - R^2} = \notag \\
	 		&= -\frac1{2\sigma^2}\cbr{R^2 + 2\sigma^2 c\sum_{i: \norm{\mb x_i - \mb a} > R} \cbr{\norm{\mb x_i - \mb a}^2 -R^2}} \to \max_{\mb a, R}.
	 \end{align}
	 Очевидно, задачи (\ref{ProbARX_simple}) и (\ref{min:noprob}) эквивалентны при $C = 2\sigma^2 c.$


\section{Решение оптимизационной задачи} 	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%		\input{solvebasic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Итак для нахождения значений $\mb a$ и $R$ необходимо решить следующую задачу
	\begin{equation}
	\label{min:main}
				\begcas{
				&R^2 + C\suml_i\xi_i \to \min\limits_{\mb a, R,\mb\xi}, \\
				&\norm{\mb x_i-\mb a}^2\le R^2 + \xi_i,\quad\xi_i\ge0,\quad i = 1,\ldots,N.
				} 
	\end{equation}
	Функция Лагранжа этой задачи имеет вид
	\begin{multline*}
		\Ell(\mb a, R,\mb\xi,\mb\alpha,\mb\gamma)=
			R^2 + C\suml_i\xi_i - \suml_i\gamma_i\xi_i  -\\
		-\suml_i\alpha_i\cbr{R^2 + \xi_i - \cbr{\mb x_i\cdot\mb x_i - 2\mb a\cdot\mb x_i + \mb a\cdot\mb a}},
	\end{multline*}
	где $\alpha_i\ge 0$  и $\gamma_i\ge 0$ --- множители Лагранжа.
	Необходимым условием минимума является равенство нулю частных производных функции Лагранжа по всем переменным
	\begin{equation}
		\label{minLag}
		\begin{array}{ll}
			\dd \Ell R = 0: & \suml_i \alpha_i = 1 \:\cbr{\text{случай $R = 0$ рассмотрим отдельно,}}\\
			\dd \Ell{\mb a} = 0: & \mb a = \cfrac{\suml_i\alpha_i\mb x_i}{\suml_i \alpha_i}= \suml_i \alpha_i\mb x_i,\\
			\dd \Ell{\xi_i} = 0: & \gamma_i = C - \alpha_i, \;\; i = 1,\ldots,N.
		\end{array}
	\end{equation}

	Из последнего уравнения получаем, что $\alpha_i = C - \gamma_i$. 
	Таким образом, мы получаем новые ограничения на $\alpha_i$ 
	$$0\le\alpha_i\le C, \;\; i = 1,\ldots,N.$$

	Если это ограничение выполнено, то мы можем вычислить $\gamma_i$ по формуле $\gamma_i = C - \alpha_i$, и при этом автоматически будет выполнено условие $\gamma_i\ge 0$.

	Тогда для функции Лагранжа получим выражение
	{
	\newcommand\ai[0]{\alpha_i\,}
	\newcommand\aj[0]{\alpha_j\,}
	\newcommand\mbx[1]{\mb x_#1}
	\newcommand\cd[0]{\!\cdot\!}
	\al{
	\Ell(\mb a, R,&\mb\xi,\mb\alpha,\mb\gamma)
		= 	R^2 - \suml_i\ai R^2 + C\suml_i\xi_i - \suml_i\ai\xi_i  +\\
		&+ \suml_i\ai\mbx i\cd\mbx i -  2\suml_i\ai\mb a\cd\mbx i + \suml_i\ai\mb a\cd\mb a - \suml_i\gamma_i\xi_i =\\
		&= R\cd R\cd\cbr{1-\suml_i\ai} +\suml_i\xi_i\cbr{C-\ai-\gamma_i} + \\
		&+ 	\suml_i\ai\mbx i\cd\mbx i - 2 \suml_i\ai\suml_j\aj\mbx j\cd\mbx i
		+ 	\suml_{i,j}\ai\aj \mbx j\cd \mbx i = \\
		&= 	\suml_i\ai\mbx i\cd\mbx i-\suml_{i,j}\ai\aj\mbx j\cd\mbx i \to \max_{\mb\alpha}.
	}
	}

	Полученное выражение является квадратичной формой. 
	Тогда его максимум находится по известным алгоритмам решения задач квадратичного программирования. 
	По оптимальным значениям $\mb{\alpha}$ мы сможем найти оптимальное значение центра гиперсферы $\mb a$ и отступов $\mb{\xi}$ используя соотношения (\ref{minLag}). 
	Те векторы $\mb x_i$, для которых $\alpha_i=0$ и $\gamma_i=C$, лежат внутри гиперсферы, те , для которых $0<\alpha_i<C$ и $0<\gamma_i<C$~--- на её границе, а те, для которых $\alpha_i=C$ и $\gamma_i=0$, лежат вне гиперсферы и имеют ненулевой отступ $\mb\xi$. 
	Радиус $R$ определяется как расстояние от центра гиперсферы $\mb a$ до опорных векторов лежащих на границе гиперсферы.

	Если же $R = 0$, то задача (\ref{min:main}) имеет вид 
	\begin{equation}
				\begcas{
				&C\suml_i\xi_i \to \min\limits_{\mb a, \mb\xi}, \\
				&\norm{\mb x_i-\mb a}^2\le \xi_i,\quad\xi_i\ge0,\quad i = 1,\ldots,N.
				} 
	\end{equation}
	т.е.
	\begin{equation}
				C\suml_i\norm{\mb x_i-\mb a}^2 \to \min\limits_{\mb a},
	\end{equation} 
	а эта задача соответствует методу наименьших квадратов. Тогда $\mb a = \frac{\sum_i \mb x_i}N$. 
	При этом следует понимать, что значение $R=0$ обнуляет обобщающую способность нашего классификатора, поэтому следует отказываться от такого решения, если есть выбор. 
	Здесь же стоит отметить, что $R = 0$ обязательно, если $C < \frac1N,$ где $N$~--- число объектов в обучающей выборке, поскольку в этом случае условия на $\mb \alpha$ несовместны.

	Для возможности описания данных более гибкой формой, нежели сфера, в работе \cite{Tax2001} предлагается использовать потенциальные функции \cite{Izerman1979}. Наиболее часто используемыми потенциальными функциями являются полиномиальная
	$$K_p(\mb x_i, \mb x_j) = \cbr{1 + \mb x_i \cdot \mb x_j}^p$$
	и радиальная базисная функция Гаусса
	$$K(\mb x_i, \mb x_j) = \exp\cbr{-\frac{\norm{\mb x_i - \mb x_j}^2}{2s^2}}.$$
	Таким образом, чтобы получить улучшенную модель описания данных, необходимо заменить в функции Лагранжа операцию вычисления
	скалярного произведения двух векторов вычислением значения потенциальной функции двух аргументов.


\section{Численный эксперимент} 		 	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%		\input{numerical}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Для оценки качества работы алгоритма предлагается ввести метрику. 
	Следуя работе  \cite{Romanenko2012}, будем измерять качество одноклассовой класификации в терминах точности и полноты. 
	В нашем случае точность (precision)~--- доля верно классифицированных объектов тестовой выборки среди всех объектов, отнесенных алгоритмом к единственному классу. 
	Полнота (recall)~--- доля верно классифицированных объектов тестовой выборки среди всех объектов, принадлежащих к единственному классу. 
	Более высокие значения точности и полноты соответствуют лучшему качеству классификации. 
	В качестве агрегированного показателя, объединяющего точность $P$ и полноту $R$ используем $F_1$-меру \cite{Rijsbergen1979}:
	$$F_1 = \frac{2PR}{P+R}.$$

	Для проведения вычислительного эксперимента сгенерируем $N=400$ случайных точек $\fbr{\mb x_i}_{i=1}^N$ из распределения (\ref{PhiXARC}) при размерности пространства $2$ (для наглядности), положив направления смещений случайными и придав параметрам значения $a = \cbr{1,2}\T, R = 3, c = 0{,}2.$
	После этого проведем $t{\times}q$-fold кросс-валидацию с $t = 10, q = 3$, скользящим контролем подбирая параметр $C$ и вычисляя $F_1$-метрику при каждом его значении. При этом всё, что лежит вне сферы мы считаем не принадлежащим классу, а всё, что внутри,~--- считаем.
	В результате получим следующую зависимость значения метрики от $C$ (см. рисунок \ref{eps:CrValLong}).
	Из графика видно, что при $C\to 0$ обобщающая способность также стремится к нулю, поскольку практически отсутствует штраф за непопадание в класс при обучении. 
	При этом большие штрафы заставляют необоснованно увеличивать сферу, снижая точность.

	Пример работы алгоритма приведен на рисунке \ref{eps:example} при параметре $C = 0{,}007.$ 
	Зеленым изображена граница истинного распределения, красным~--- построенного.
	Видно, что здесь $C$ слишком мало и сфера получилось слишком маленькой, поскольку штраф за .
	
	\begin{figure}[!ht] %lrp
		\centering
		\includegraphics[height=240px]{CrValLong.eps} % CV_0.000-0.010-0.200_T10_Q3.eps
		\vspace{-5pt}
		\caption{Модельные данные. Зависимость $F_1$-метрики от параметра регуляризации $C$.}
		\label{eps:CrValLong}
	\end{figure}

	\begin{figure}[!ht] %lrp
		\centering
		\includegraphics[height=240px]{example.eps} 
		\vspace{-5pt}
		\caption{Пример результата работы алгоритма при $C = 0{,}007.$}
		\label{eps:example}
	\end{figure}

	Для проведения эксперимента на реальных данных были выбраны доступные в открытом доступе уже вычисленные признаки сообщений \footnote{UCI Machine Learning Repository \href{http://archive.ics.uci.edu/ml/datasets/Spambase}{http://archive.ics.uci.edu/ml/datasets/Spambase}}. 
	Здесь для обучения бралась небольшая часть спам-документов (200 из ~1800). 
	Сперва они линейно отображались в куб $[0, 1]^k$ ($k = 57$~--- размерность пространства), а затем по ним строилась сфера в этом 57-мерном пространстве. 
	Для контроля все остальные данные преобразовывались по тому же правилу (что не гарантирует их попадание в этот же куб), после чего проверялось попадание в построенную сферу и вычислялась $F_1$-метрика. 
	Здесь в контроле уже участвуют объекты как объекты из исследуемого класса (спам-сообщений), так и не из него, хотя обучение происходило только на объектах целевого класса. 
	Результаты подбора параметра $C$ изображены на рисунке (\ref{eps:CVReal}). Данные усреднены по 20 случайным выборкам по 200 объектов из 1800.

	\begin{figure}[!ht] %lrp
		\centering
		\includegraphics[height=240px]{CVReal.eps} % Real_N200_0.0300-0.0050-0.1500_T20.eps
		\vspace{-5pt}
		\caption{Реальные данные. Зависимость $F_1$-метрики от параметра регуляризации $C.$}
		\label{eps:CVReal}
	\end{figure}

	В обоих экспериментах отчетливо прослеживаются максимумы метрики, что свидетельствует о наличии в обеих задачах оптимального значения параметра $C.$


\section{Заключение}						
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%		\input{conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	В работе был предложен вероятностный подход к задаче одноклассовой классификации. Такой подход более удобен, чем классический подход, основанный на эвристических соображениях, и с теоретической, и с практической точек зрения, поскольку несет в себе ясную возможность модификаций. При этом классический подход является частным случаем предложенного алгоритма.

	В работе построен алгоритм описания классов шарами, проведено его обобщения на случай ядерных функций. Проведены вычислительные эксперименты на модельных и реальных данных.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%		\input{bibliography.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}
	\bibitem{Tax2001} D.\,Tax  \textit{One-class classification; Concept-learning in the absence of
	counterexamples}, Ph.D thesis, 2001.
	\bibitem{Khan2006} S.\,Khan, G.\,Madden\textit{A Survey of Recent Trends in One Class
	Classiffcation}, College of Engineering and Informatics, National University of Ireland Galway,
	Ireland, 2006.
	\bibitem{Islam2007} R.\,Islam, U.\,Chowdhury \textit{Spam filtering using ML algorithms}, Universitetets Okonomiske Institute, IADIS International Conference on WWW/Internet, 2007.
	\bibitem{Sun2008} J.\,Sun, Q.\,Zhang, Z.\,Yuan, W.\,Huang, X.\,Yan, J.\,Dong \textit{Research of Spam Filtering system based on LSA and SHA}, Advances in neural networks - ISNN 2008, 2008.
	\bibitem{Romanenko2012} А.\,Романенко  \textit{Категоризация текстов на основе монотонного
	классификатора ближайшего соседа}, Выпускная квалификационная работа бакалавра, 2012.
	\bibitem{Rijsbergen1979} C.\,J. van Rijsbergen, Information Retrieval (2nd ed.), Butterworth, 1979.
	\bibitem{Groot1974}М.\,Де Гроот \textit{Оптимальные статистические решения}, Москва, «Мир», 1974
	\bibitem{Izerman1979} М.\,А.\,Айзерман, Э.\,М.\,Браверман, Л.\,И.\,Розоноэр \textit{Метод потенциальных функций в теории обучения машин}, Москва, <<Наука>>, 1970.
\end{thebibliography}

\end{document}
