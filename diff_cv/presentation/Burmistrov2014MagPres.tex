\documentclass[unicode,lefteqn,c,hyperref={pdfpagelabels=false}]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amssymb}
\usepackage{amsmath,mathrsfs}
\usepackage[russian]{babel}
\usepackage{ulem}\normalem
\usepackage{color}
\usepackage[noend]{algorithmic}
\usepackage[outdir=./]{epstopdf}

\input ../macro.tex

\usetheme{Warsaw}
\usefonttheme[onlylarge]{structurebold}
\setbeamerfont*{frametitle}{size=\normalsize,series=\bfseries}
\setbeamertemplate{navigation symbols}{}
\setbeameroption{show notes}
\setbeamertemplate{itemize items}[default]
\definecolor{beamer@blendedblue}{RGB}{15,80,120}
\let\Tiny=\tiny
\def\shortspace{\hspace{1.5pt}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[\hbox to 56mm{Дифференциальный скользящий контроль \hfill\insertframenumber\,/\,\inserttotalframenumber}]{Дифференциальный скользящий контроль при выборе параметров регуляризации в задаче регрессионного анализа с помощью Elastic Net}
\author[М.\shortspaceО.\shortspaceБурмистров]{М.\shortspaceО.\shortspaceБурмистров}
\institute{Научный руководитель д.т.н.
	\vfill В.\,В.~Моттль \vfill ~
	\vfill Московский физико-технический институт
	\vfill Факультет управления и прикладной математики
	\vfill Кафедра интеллектуальных систем}
\date{10 июня 2014\,г.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frame}
	\titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Введение}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Исходные данные}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Постановка задачи линейной регрессии}
	\textbf{Изучаемые объекты:}
	\begin{itemize}
			\item $\Omega$~--- объекты реального мира;
			\item $\mb x \colon \Omega \to X = \mathbb R^n$~--- признаковое описание объектов, $\mb x(\omega) = (x_1, \ldots, x_n)^T$;
			\item $y \colon \Omega \to Y = \mathbb R$~--- скрытая характеристика объектов.
	\end{itemize}

	\smallskip
	% \rlap{\textbf{Основные гипотезы вероятностной тематической модели:}}
	\textbf{Исходные данные}
	\begin{itemize}
		\item $\Omega^* = \fbr{w_i\cond i=1,\ldots,N}\subset \Omega$~--- обучающая совокупность;
		\item $\mb X = (\mb x_1, \ldots, \mb x_N)^T$~--- матрица объекты-признаки;
		\item $\mb y = (y_1, \ldots, y_N)^T$~--- вектор ответов.
	\end{itemize}

	\smallskip
	\textbf{Задача линейной регрессии:}

	Функция $\hat y \colon X \to Y$ ищется в параметрическом семействе
	\begin{equation*}
		\hat y(\mb x) = \mb a^T\mb x + b, \:\text{где $\mb a \in \mathbb R^n$ и $b\in \mathbb R.$}
	\end{equation*}
\end{frame}

\subsection{Условия регуляризации Elastic Net}
\begin{frame}{Добавим регуляризацию для отбора информативных признаков}
	\vspace{-5pt}
	\textbf{Исходный функционал качества}
	\vspace{-5pt}
	\begin{equation*}
		\suml_{i=1}^N(y_i - \hat y(\mb x_i))^2.
	\end{equation*}

	\smallskip
	\textbf{Регуляризация Elastic Net}

	Для отбора признаков введём штрафную функцию
	\begin{equation*}
		\beta\suml_{i=1}^n a_i^2 + \mu\suml_{i=1}^n\abs{a_i}.
	\end{equation*}

	\smallskip
	\textbf{Требования нормировки}

	При использовании общих коэффициентов штрафа для компонент \mb a необходимо сделать замену, чтобы обучающая совокупность была центрирована и нормирована:
	\begin{equation*}
		\frac1N\suml_{i=1}^N\mb x_i = \mb 0, \:
		\frac1N\suml_{i=1}^N y_i = 0, \:
		\foral{j=1,\ldots, n} \: \frac1N\suml_{i=1}^N\mb x_{ij}^2 = 1.
	\end{equation*}
\end{frame}

\def\lams{{\mu,\beta}}
\section{Поиск оптимальных параметров регуляризации}
\subsection{Быстрая кросс-валидация}
\begin{frame}{Выделение структурных параметров}
	\textbf{Задача оптимизации}
	\begin{equation*}
		\begin{cases}
			\suml_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i}}
			+ \suml_{j=1}^N \delta_j^2 \to \min_{\mb a}, \\
			\delta_j = y_j - \suml_{i=1}^nx_{ij}a_i, j=1,\ldots,N.
		\end{cases}
	\end{equation*}
	Оптимальный вектор $\hat{\mb a}$ зависит от $\lams, \Omega^*.$

	При этом множество признаков $I$ естественным образом разбивается на 3 подмножества:
	\begin{equation*}
		\hat I^-_\lams = \fbr{i\in I: \hat a_{i} < 0},
		\hat I^0_\lams = \fbr{i\in I: \hat a_{i} = 0},
		\hat I^+_\lams = \fbr{i\in I: \hat a_{i} > 0}.
	\end{equation*}

	\smallskip
	\textbf{Кросс-валидация}

	\begin{equation*}
		S_{LOO}(\lams) = \frac1N\suml_{i=1}^N\cbr{y_i - \hat{\mb a}^T(\Omega \backslash \fbr{\omega_i})\mb x_i}^2 \to \min_{\lams}
	\end{equation*}
\end{frame}

\begin{frame}{Устойчивость структурного параметра}
	Зафиксируем множество $\hat I_\lams = \hat I^-_\lams \cup I^+_\lams$ и предположим, что оно не изменяется при исключении любого одного объекта $\omega_i$ из обучающей совокупности $\Omega^*.$

% \def\A{\tilde{\mb X}^T\tilde{\mb X} + \beta \tilde{\mb I}_{\hat n}}
% \def\xxt{\tilde{\mb x}_k \tilde{\mb x}_k^T}
% \def\denominator{1 - \tilde{\mb x}_k^T\inv{\A}\tilde{\mb x}_k}
% \def\besta{\hat{\tilde{\mathbf a}}}
	\textbf{Теорема об оценке функционала качества:}
		\begin{equation*}
			S_{LOO, \hat I_\lams}
			= \frac1N\suml_{k=1}^N
			\cbr{
			\frac{
				y_k - \hat y_k
			}{
				1 -
				\tilde{\mb x}_k^T
				\inv{\tilde{\mb X}^T\tilde{\mb X} + \beta \tilde{\mb I}_{\hat n}}
				\tilde{\mb x}_k
			}
			}^2, \text{где}
		\end{equation*}
		
		\vspace{-10pt}
		\begin{align*}
			\hat n &= \abs{\hat I_\lams} \text{ --- число ненулевых признаков,}\\
			\hat{\tilde{\mathbf a}} &= (\hat a_i \cond i\in \hat I_\lams)^T \text{ --- ненулевые компоненты $\hat{\mb a}(\Omega^*),$} \\
			\tilde{\mb x} &= (x_i \cond i\in \hat I_\lams)^T,
			\tilde{\mb X}\!=\!(\tilde{\mb x}_1, \ldots, \tilde{\mb x}_N)\!^T \text{ --- значения признаков $\hat I_\lams,$} \\
			\hat y_k &= \hat{\tilde{\mathbf a}}^T\!(\Omega^*)\tilde{\mb x}_k = \hat{\mb a}^T\!(\Omega^*)\mb x_k \text{ --- значения функции регрессии.}
		\end{align*}
		\vspace{-20pt}
\end{frame}

\subsection{Дифференциальная кросс-валидация}
\begin{frame}{Добавление весов объектам обучающей совокупности}
	Изменим задачу оптимизации, присвоив вес $p$ каждому объекту обучения $\omega$:
	\begin{equation*}
		\begin{cases}
			\suml_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i}}
			+ \suml_{j=1}^N p_j\delta_j^2 \to \min_{\mb a}, \\
			\delta_j = y_j - \suml_{i=1}^nx_{ij}a_i, j=1,\ldots,N.
		\end{cases}
	\end{equation*}

	Функционал ошибки оставим прежним:
	\begin{equation*}
		S(\mb p, \lams) = \frac1N\suml_{k=1}^N \hat{\delta}_k^2(\mb p, \lams).
	\end{equation*}

	\vspace{-5pt}
		\textbf{Лемма о производной регрессионных остатков:}
		\vspace{-5pt}
		\begin{equation*}
			\dd{\hat\delta_k}{p_k} = - \tilde {\mb x}_k^T
			\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
			\tilde{\mb x}_k \hat \delta_k, \text{ где } P = \text{diag}\cbr{p_1, \ldots, p_N}.
		\end{equation*}
\end{frame}

\begin{frame}{Принцип дифференциальной кросс-валидации}
	Обучение на всех объектах соответствует единичному вектору $\mb p = \mb e = (1, \ldots, N)^T.$
	Кросс-валидация же при этом соответствует поочерёдному обнулению компонент вектора
	$\mb p^{(i)} = \mb e^{(i)} = (e^{(i)}_1, \ldots, e^{(i)}_N)^T, e^{(i)}_k=\begcas{1, i\ne k, \\ 0, i = k.}$

	\textbf{Использование в качестве критерия кросс-валидации приближение функции ошибки:}
	\begin{align*}
		S_{Diff}(\lams)
		&= \frac1N\suml_{k=1}^N \hat{\delta}_k^2(\mb e, \lams)
		+ \suml_{k=1}^N\left.\dd{\hat{\delta}_k^2(\mb p, \lams)}{p_k}\right|_{\mb p = \mb e}\cdot(-1) = \\
		&= \frac1N\suml_{k=1}^N
		\hat{\delta}_k^2(\mb e, \lams)
		\cbr{1 + 2\tilde {\mb x}_k^T \inv{\tilde{\mb X}^T \tilde{\mb X} + \beta I_{\hat n}}\tilde{\mb x}_k}.
	\end{align*}
\end{frame}

\subsection{Сравнение результатов}
\begin{frame}{}
	Положим $\delta^k = y_k - \hat y_k$~--- регрессионные остатки при обучении на $\Omega^*$.
	Тогда
	\begin{align*}
		S_{Diff}(\lams)
			&= \frac1N\suml_{k=1}^N
			\hat{\delta}_k^2
			\cbr{1 + 2\tilde {\mb x}_k^T \inv{\tilde{\mb X}^T \tilde{\mb X} + \beta I_{\hat n}}\tilde{\mb x}_k}, \\
		S_{LOO, \hat I_\lams}(\lams)
			&= \frac1N\suml_{k=1}^N
				\hat{\delta}_k^2
				\cbr{
				1 -
				\tilde{\mb x}_k^T
				\inv{\tilde{\mb X}^T\tilde{\mb X} + \beta \tilde{\mb I}_{\hat n}}
				\tilde{\mb x}_k
				}^{-2}.
	\end{align*}
	Результаты совпадают с точностью до малых второго порядка, что доказывает эффективность применения дифференциальной кросс-валидации.
\end{frame}

\newcommand\addcolumn[3]{
	\column{0.33\textwidth}
		\includegraphics[height=95px]{../graph/#1.png}
		\vspace{10pt}
		\begin{center}
			\vspace{-25pt}
			$#2$
			\vspace{-5pt}
			\begin{align*}
			#3.
			\end{align*}
		\end{center}
}

\section{Численный эксперимент}
\subsection{Модельные данные}
\begin{frame}{Сгенерированная выборка из 100 объектов}
	\vspace{-20pt}
	\begin{align*}
		x_{ij} &\sim Exp(j), \: i=1,\ldots,100, \: j=1,\ldots,5, \: \mb a = (0, 2, 0, -3, 0)^T, \\
		y_{i} &= \mb x_i^T \mb a + \eps_i, \eps_i \sim Exp(j), i=1,\ldots,100. 
	\end{align*}
	\vspace{-15pt}
	\begin{columns}
		\hspace{-15pt}
		\addcolumn{LOO_I}{S_{LOO, \hat I_\lams}(\lams)}{
			\hat S_{LOO, \hat I} &=0.0136, \\ \hat \beta &=0.156, \\ \hat \mu &=47.75
		}
		\addcolumn{diff_CV}{S_{Diff}(\lams)}{
			\hat S_{Diff} &=0.0119, \\ \hat \beta &=0.160, \\ \hat \mu &=48.55
		}
		\addcolumn{honest_CV}{S_{LOO}(\lams)}{
			\hat S_{LOO} &=0.0119, \\ \hat \beta &=0.159, \\ \hat \mu &=48.70
		}
	\end{columns}
\end{frame}

\section*{}
\subsection{Заключение}
\begin{frame}{Заключение, результаты и выводы}
\begin{itemize}
	\item Получены 2 независимых подхода к беспереборной оценке функционала качества в кросс-валидации при использовании ограничений регуляризации Elastic Net.
	\item Обоснована их эквивалентность между собой, а также традиционной оценке.
	\item Экспериментально изучена работа обоих подходов на модельных 
	% и реальных 
	данных, показана эффективность применения дифференциальной кросс-валидации.
\end{itemize}
\end{frame}

\end{document}
