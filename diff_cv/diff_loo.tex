\subsection{Оценка ошибки LOO}
Ранее в пункте (\ref{sub:intro:weight}) было показано, как LOO-кросс-валидацию можно описать в терминах взвешенных объектов.
Получим оценку скользящего контроля в следующей модели обучения. Предположим, что один раз обучившись при единичном векторе весов $\mb p = \mb e$ и найдя вектор $\hat{\mb a}(\mb e, \lams),$ мы получили множества $I^+, I^-, I^0,$ а затем будем поочерёдно присваивать объектам нулевой вес, при этом сохраняя фиксированными разбиение множества индексов. Тогда задача оптимизации будет оставаться квадратичной и её решение может быть проведено эффективнее, с использованием утверждения (\ref{stmnt:locally_linear}):
\begin{equation}
	\label{LOO}
	S_{LOO} = \frac1N\suml_{k=1}^N \cbr{y_k - \mb x_k^T\hat{\mb a}(\beta, \mu, \mb e^{(k)})}^2.
\end{equation}
% \begin{align}
% 	\hat S(\lams) &= \frac1N\sum_{j=1}^N \hat\delta_{j, \lams}^2, \\
% 	\hat \delta_{j, \lams} &= y_j - \mb x_j^T \hat {\mb a}_\lams.
% \end{align}
% В используемых обозначениях кросс-валидацию можно рассматривать как обучение с поочерёдным присвоением объектам нулевых весов ($p_j=0$).
% Усреднённые квадратичные остатки

% \begin{align}
% 	\hat{S}_{\text{LOO}}(\lams)&=\frac1N\sum_{j=1}^N\sqr{\hat{\delta}_{j, \lams}^{(j)}}, \\
% 	\hat \delta_{j, \lams}^{(j)} &= y_j - \mb x_j^T \hat {\mb a}_\lams^{(j)}.
% \end{align}

% $\hat {\mb a}_{\lams}^{(j)}$~--- результат обучения при векторе весов $p^{(j)}, p_i^{(j)} = \sbr{i \ne j}$ (единицы на всех позициях, кроме $j$-ой), т.е. при исключённом объекте $j.$

% \begin{equation}
% 	\hat \delta_{j, \lams}^{(j)}
% 		= y_j - \mb x_j^T \hat {\mb a}_\lams^{(j)}
% 		= y_j - \suml_{i\inotnull} x_{ji} \hat{\mb a}_{i, \lams}^{(j)}
% 		= y_j - \tilde{\mb x}_j^T \besta_\lams^{(j)}.
% \end{equation}

\begin{statement}
\label{stmnt:CVI}
При фиксированном разбиении множества индексов оценка скользящего контроля может быть вычислена в виде
\begin{equation*}
	S_{LOO, I^+, I^-} = \frac1N\suml_{k=1}^N\frac{y_k - \mb x_k^T\hat{\mb a}(\lams, \mb e)}{\denominator}.
\end{equation*}
\end{statement}

\begin{Proof}
\def\this{^{(k)}}
В течение доказательства опустим индексы $\lams$ для упрощения обозначений. 
Верхний индекс ${}\this$ будем добавлять объектам, которые построены по всем объектам обучающей совокупности, кроме $\omega_k$, аналогично добавлению $\tilde{\:}$ над матрицами и векторами, из которых исключены компоненты, соответствующие нулевым элементам вектора $\hat{\mb a}.$

Согласно утверждению (\ref{stmnt:locally_linear})

\begin{equation*}
	\besta^{(k)}
		= \cbr{\cbr{\tilde{\mb X}\this}^T \tilde{\mb X}\this + \beta \tilde{\mb I}_{\hat n_\lams}}^{-1}\cbr{\cbr{\tilde{\mb X}\this}^T \mb y^{(k)} -\frac\mu2\tilde{\mb e}}.
\end{equation*}

Используя соотношения
\begin{align*}
	\cbr{\tilde {\mb X}\this}^T \tilde{\mb X}\this
		&= \tilde{\mb X}^T \tilde{\mb X} - \tilde{\mb x}_k \tilde{\mb x}_k^T, \\
	\cbr{\tilde {\mb X}\this}^T \mb y^{(k)}
		&= \tilde{\mb X}^T \mb y - y_k \tilde{\mb x}_k
\end{align*}
и матричное равенство Вудбери
\begin{equation*}
	\inv{\mb A+\mb B \mb C} = \mb A^{-1} - \mb A^{-1} \mb B\inv{\mb I + \mb C \mb A^{-1} \mb B} \mb C \mb A^{-1},
\end{equation*}

получаем
\begin{align*}
	\besta\this
		&= \inv{\A + \cbr{- \xxt}}
		\cbr{\tilde{\mb X}^T \mb y - y_k\tilde{\mb x}_k -\frac\mu2\tilde{\mb e}} = \\
		&= \inv{\A} \cbr{\tilde{\mb X}^T P^2\mb y - y_k\tilde{\mb x}_k -\frac\mu2\tilde{\mb e}} - \\
		&- \frac{\inv{\A}\cbr{-\xxt}\inv{\A}\cbr{\tilde{\mb X}^T \mb y  -\frac\mu2\tilde{\mb e} - y_k\tilde{\mb x}_k}}{\denominator} = \\
		&= \besta + \frac{\inv{\A}\xxt\besta}{\denominator}
		- y_k\frac{\inv{\A}}{\denominator} \cdot \\
		&\cdot \fbr{\tilde{\mb x}_k\cbr{\denominator} + \xxt\inv{\A}\tilde{\mb x}_k} = \\
		&= \besta + \frac{\inv{\A}\xxt\besta}{\denominator} - y_k\frac{\inv{\A}}{\denominator}\tilde{\mb x}_k.
\end{align*}

Выразим результаты $\hat y\this_k = \tilde{\mb x}^T_k\besta\this$ классификации при исключённом $k$-м объекте через результат классификации $\hat y_k = \tilde{\mb x}^T_k\besta$ при использовании всей обучающей совокупности:

% УБРАТЬ все Р - это настоящий ЛОО

\begin{align*}
	\hat y\this_k
		&= \hat y_k + \hat y_k\frac{\tilde{\mb x}_k^T\inv{\A} \tilde{\mb x}_k}{\denominator}
		- y_k\frac{\tilde{\mb x}^T_k\inv{\A}\tilde{\mb x}_k}{\denominator} = \\
		&= \frac{\hat y_k}{\denominator} - y_k\frac{\tilde{\mb x}^T_k\inv{\A}\tilde{\mb x}_k}{\denominator}.
\end{align*}

Тогда регрессионные остатки окажутся равными
\begin{align*}
	\hat{\delta}_k\this
	&= y_k - \hat y\this_k = y_k - \frac{\hat y_k}{\denominator} + y_k\frac{\tilde{\mb x}^T_k\inv{\A}\tilde{\mb x}_k}{\denominator} = \\
	&= \frac{y_k - \hat y_k}{\denominator}.
\end{align*}

Доказательство завершено.
\end{Proof}

При этом отметим, что предположение о неизменности разбиения множества индексов является весьма сильным, 
поэтому далее будет рассмотрен подход, позволяющий не полностью исключать объекты из выборки, изменяя вес с 1 на 0, 
а лишь варьировать его в окрестности 1.

\subsection{Дифференциальный подход}
Положим 
\begin{align*}
	&S(\mb p, \lams) = \frac1N\suml_{k=1}^N \delta_k^2, \\
	&\text{где } \delta_k = y_k - \mb x_k^T\hat{\mb a}(\mb p, \lams).
\end{align*}
\begin{theorem}
\label{thm:diffLOO}
\begin{equation*}	
	\left.\fbr{
		\hat S(\mb p, \lams)
	- 	\suml_{k=1}^N\dd{\delta_k^2(\mb p, \lams)}{p_k}
	}\right|_{\mb p=\mb e}
	= 	\frac1N\suml_{k=1}^N
			\hat \delta_{k, \lams}^2
		\cbr{
			1
		+ 	2
			\tilde {\mb x}_k^T
			\inv{\tilde{\mb X}^T \tilde{\mb X} + \beta I_{\hat n}}
			\tilde{\mb x}_k
		}
\end{equation*}
\end{theorem}

\begin{Proof}
% \begin{align}
% 	\hat S(\lams)
% 	&= \frac1N\suml_{k=1}^N\hat\delta_{k, \lams}^2
% 	= \frac1N\suml_{k=1}^N\cbr{y_k - \tilde {\mb x}_k^T \besta}^2 = \\
% 	&= \frac1N\suml_{k=1}^N
% 		\cbr{y_k - \tilde {\mb x}_k^T
% 		\inv{\tilde{\mb X}^T_\lams P^2\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}}
% 		\cbr{\tilde{\mb X}^T_\lams P^2 y - \frac\mu2\tilde{\mb e}_\lams}
% 	}^2.
% \end{align}

% Положим $\mb \Delta = \mb P^2 - \mb I = \diag{p_1 - 1, \ldots, p_N - 1},$ тогда
\begin{align*}
	\dd{\hat\delta_k(\mb p \cond \lams)}{p_j}
	&= 	\dd{}{p_j}
		\fbr{
			y_k - \tilde {\mb x}_k^T \inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
			\cbr{\tilde{\mb X}^T P \mb y -\frac\mu2\tilde{\mb e}}
		} = \\
	&= - \tilde {\mb x}_k^T
	\left\{
		\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
	  	\dd{\cbr{\tilde{\mb X}^T P^2 \mb y}}{p_j} -
	  	\right. \\
	&- 	\left.
	   	\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
		\dd{\cbr{\tilde{\mb X}^T P\tilde{\mb X}}}{p_j}
		\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
		\cbr{\tilde{\mb X}^T P \mb y -\frac\mu2\tilde{\mb e}}
	\right\} = \\
	&= - \tilde {\mb x}_k^T
	\fbr{
		\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_jy_j
		-
		\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_j \tilde{\mb x}^T_j
		\besta
	}
\end{align*}

\begin{align*}
	\dd{\hat\delta_k}{p_k}
	&= - \tilde {\mb x}_k^T
	\fbr{
		\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_ky_k
		-
		\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_k \tilde{\mb x}^T_k
		\besta
	} = \\
	&= -\tilde {\mb x}_k^T
		\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_k \cbr{y_k - \hat y_k}
	= - \tilde {\mb x}_k^T
		\inv{\tilde{\mb X}^T P\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_k \hat \delta_k.
\end{align*}

Тогда
\begin{align*}
	\left.\fbr{
		\hat S(\mb p, \lams)
	- 	\suml_{i=1}^N\dd{\delta_i^2(\mb p, \lams)}{p_i}
	}\right|_{\mb p=\mb e}
	&= 	\frac1N\suml_{i=1}^N
		\cbr{
			\hat \delta_{i, \lams}^2
		+ 	2 \delta_i(\mb p, \lams)
			\tilde {\mb x}_i^T
			\inv{\tilde{\mb X}^T \tilde{\mb X} + \beta I_{\hat n}}
			\tilde{\mb x}_i \hat \delta_i
		} = \\
	&= 	\frac1N\suml_{i=1}^N
			\hat \delta_{i, \lams}^2
		\cbr{
			1
		+ 	2
			\tilde {\mb x}_i^T
			\inv{\tilde{\mb X}^T \tilde{\mb X} + \beta I_{\hat n}}
			\tilde{\mb x}_i
		}
\end{align*}
Теорема доказана.
\end{Proof}

% Изучим, как соотносятся
% $\suml_{j=1}^N\sqr{\hat{\delta}_{j, \lams}^{(j)}}$
% и $\suml_{j=1}^N \hat\delta_{j, \lams}^2$

% \begin{align}
% 	\suml_{j=1}^N\sqr{\hat{\delta}_{j, \lams}^{(j)}} &=
% 		\suml_{j=1}^N\sqr{y_j - \mb x_j^T \hat {\mb a}_\lams^{(j)}} \\
% 	\suml_{j=1}^N \hat\delta_{j, \lams}^2 &=
% 		\suml_{j=1}^N \sqr{y_j - \mb x_j^T \hat {\mb a}_\lams}
% \end{align}

% % \end{proof}
% % \newtheorem{о решении задачи оптимизации}{Решение $\hat a_{\lams}$ задачи оптимизации (\ref{LagrWEN}) является комбинацией решения }

% \begin{align}
% 	% \dd{}{\mb p}\hat{S}_{\lambda_1, \lambda_2}&= \frac2N\cbr{y_j - x_j^T \dd{}{\mb p}\hat {\mb a}_{\lambda_1, \lambda_2}}.
% 	\dd{}{p_k}\hat{S}_{\lams}&= \frac2N\sum_{j=1}^N\cbr{y_j - x_j^T \dd{}{p_k}\hat {\mb a}_{\lams}}
% \end{align}


\subsection{Сравнение результатов} % (fold)
Преобразуем (\ref{LOO})
\begin{align*}
% \frac1N\suml_{k=1}^N \cbr{y_k - \mb x_k^T\hat{\mb a}(\beta, \mu, \mb e^{(k)})}^2.
	S_{LOO}(\lams) 
	&= \frac1N\suml_{k=1}^N \hat{\delta}^2_k(\mb e^{(k)}, \lams) = \\
	&= \frac1N\suml_{k=1}^N \hat{\delta}_k^2(\mb e, \lams)
	+ \frac1N\suml_{k=1}^N \frac{\hat{\delta}_k^2(\mb e^{(k)}, \lams) - \hat{\delta}_k^2(\mb e, \lams)}1 = \\
	&= \frac1N\suml_{k=1}^N \hat{\delta}_k^2(\mb e, \lams)
	- \frac1N\suml_{k=1}^N \frac{\hat{\delta}_k^2(\mb e, \lams) - \hat{\delta}_k^2(\mb e^{(k)}, \lams)}{1-0}.
\end{align*}

Элементы второй суммы являются, отношением изменения $\hat{\delta}_k^2$ при изменении веса $p_k$ с $1$ на $0$, то есть дискретной производной.
Такой представление показывает, что классический скользящий контроль через LOO можно заменить следующим критерием:
\begin{equation*}
	S_{Diff}(\lams) 
	= \frac1N\suml_{k=1}^N \hat{\delta}_k^2(\mb e, \lams) 
	- \frac1N\suml_{k=1}^N \left.
		\dd{\hat{\delta}_k^2(\mb p, \lams)}{p_k} 
	\right|_{\mb p=\mb e}.
\end{equation*}

Таким образом мы можем сформулировать критерий дифференциальной кросс-валидации:
\begin{equation*}
	S_{Diff}(\lams) 
	= \frac1N\suml_{k=1}^N \left. \fbr{
		\hat{\delta}_k^2(\mb p, \lams)
		- \dd{\hat{\delta}_k^2(\mb p, \lams)}{p_k} 
	}\right|_{\mb p=\mb e} \to \min_{\lams}.
\end{equation*}

Согласно теореме (\ref{thm:diffLOO}), значение этого критерия может быть эффективно вычислено, поскольку для каждой пары $(\lams)$ достаточно лишь единожды получить значение $\hat{\mb a}(\lams)$.

Результаты утверждения (\ref{stmnt:CVI}) и теоремы (\ref{thm:diffLOO}) совпадают с точностью до малых второго порядка. Полного совпадения нет, поскольку оптимальное решение $\hat{\mb a}$ зависит от весов $\mb p$ не линейно, а дробно-линейно (см. утверждение (\ref{stmnt:locally_linear})), и что и вызывает расхождение.