
% \beta = \lambda_2
% \mu = \lambda_1
\def\cond{|}
\def\lams{{\mu,\beta}}
\def\inotnull{\not\in \hat I^0_\lams}
%


При поиске вектора $\hat{\mb a}$ мы получаем разбиение всех его компонент (а вместе с ними и признаков) на 3 группы: положительные, равный нулю и отрицательные. Пусть $I=\fbr{1,\ldots, n}$~--- множество всех признаков. Введём  обозначения:
\begin{align}
	\hat I^-_\lams &= \fbr{i\in I: \hat a_{i, \lams} < 0}, \\
	\hat I^0_\lams &= \fbr{i\in I: \hat a_{i, \lams} = 0}, \\
	\hat I^+_\lams &= \fbr{i\in I: \hat a_{i, \lams} > 0}.
\end{align}

Используя такие обозначения и тем самым зная знаки $\hat{\mb a}_\lams^i$, перепишем задачу (\ref{weighedENdelta}) в следующем виде:


\begin{align}
	\label{min_idx}
	J(\mb a \cond \lams)
		&= \suml_{i\inotnull}\cbr{\beta a_i^2 + \mu \modul{a_i}}
		+ \suml_{j=1}^N p_j\cbr{y_j - \suml_{i=1}^nx_{ij}a_i}^2 \to \min_{\mb a}, \\
	J(\mb a, i\inotnull \cond \lams)
		&= \suml_{i\inotnull}\beta a_i^2 + \mu \suml_{i\in \hat I^+_\lams}a_i - \mu \suml_{i\in \hat I^-_\lams}a_i
		+ \suml_{j=1}^N p_j\cbr{y_j - \suml_{i\inotnull}x_{ij}a_i}^2 \to \\
		&\to \min\cbr{a_i, i\inotnull}
\end{align}

Зная $\hat I^0_\lams,$ можно говорить о том, что $\hat{\mb a}$ - решение является комбинацией решения $\tilde{{\mb a}}$ задачи (\ref{min_idx}) и совокупности равенств $\mb a_i = 0, i\in \hat I^0_\lams$.

При этом удобно ввести обозначения, соответствующие только ненулевым компонентам $\hat{\mb a}$:
\begin{align}
	\tilde{\mb a}_\lams &= (a_i, i\inotnull) \in \mathbb{R}^{\hat n_\lams}, \\
	\tilde{\mb a}_{j,\lams} &= (x_{ij}, i\inotnull) \in \mathbb{R}^{\hat n_\lams}, \\
	\tilde{\mb X}_\lams &= (\tilde{\mb x}_1, \ldots, \tilde{\mb x}_N)^T \in \mathbb{R}^{N \times \hat n_\lams}, \\
	\tilde{\mb e}_\lams &= (\tilde e_i, i\inotnull) \in \mathbb{R}^{\hat n_\lams},
		\tilde e_i = \begcas{+1, i \in \hat I^+_\lams \\-1, i \in \hat I^-_\lams}.
\end{align}

\begin{theorem}
При известном множестве $\hat I^0_\lams$
\end{theorem}

%\begin{proof}
\begin{align}
	\dd{}{a_i}J(\mb a, i\inotnull \cond \lams)
		= 2\beta a_i + \mu e_i - 2\suml_{j=1}^N p_j\cbr{y_j - \suml_{l\inotnull}x_{lj}a_l}x_{ij} = 0
\end{align}
% Здесь есть дополнительный множитель, потерянный в публиции на кольт

Тем самым мы имеем систему линейных алгебраических уравнений на $a_i$
\begin{align}
	\beta \tilde a_i + \suml_{j=1}^N p_j\suml_{l\inotnull} x_{lj}x_{ij}\tilde a_l
		= \suml_{j=1}^N p_jy_jx_{ij} -\frac\mu2 e_i.
\end{align}

Или в матричных обозначениях,
\begin{equation}
	\cbr{\tilde{\mb X}^T_\lams P^2\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}} \tilde{\mb a}
		= \tilde{\mb X}^T_\lams P^2 y -\frac\mu2\tilde{\mb e}_\lams.
\end{equation}

Теорема доказана.

\begin{align}
	\hat S(\lams) &= \frac1N\sum_{j=1}^N \hat\delta_{j, \lams}^2, \\
	\hat \delta_{j, \lams} &= y_j - \mb x_j^T \hat {\mb a}_\lams.
\end{align}
В используемых обозначениях кросс-валидацию можно рассматривать как обучение с поочередным присвоением объектам нулевых весов ($p_j=0$).
Усреднённые квадратичные остатки

\begin{align}
	\hat{S}_{\text{LOO}}(\lams)&=\frac1N\sum_{j=1}^N\sqr{\hat{\delta}_{j, \lams}^{(j)}}, \\
	\hat \delta_{j, \lams}^{(j)} &= y_j - \mb x_j^T \hat {\mb a}_\lams^{(j)}.
\end{align}

$\hat {\mb a}_{\lams}^{(j)}$~--- результат обучения при векторе весов $p^{(j)}, p_i^{(j)} = \sbr{i \ne j}$ (единицы на всех позициях, кроме $j$-ой), т.е. при исключённом объекте $j.$
\def\besta{\hat{\tilde{\mathbf a}}}

\begin{equation}
	\hat \delta_{j, \lams}^{(j)}
		= y_j - \mb x_j^T \hat {\mb a}_\lams^{(j)}
		= y_j - \suml_{i\inotnull} x_{ji} \hat{\mb a}_{i, \lams}^{(j)}
		= y_j - \tilde{\mb x}_j^T \besta_\lams^{(j)}.
\end{equation}

\begin{theorem}
про LOO оценки
\end{theorem}

Доказательство:
В течение доказательва опустим индексы $\lams$ для упрощения обозначений.
Согласно доказанной ранее теореме
\def\this{^{(k)}}
\begin{equation}
	\besta^{k}
		= \cbr{\cbr{\tilde{\mb X}\this}^T P^2\tilde{\mb X}\this + \beta \tilde{\mb I}_{\hat n_\lams}}^{-1}\cbr{\cbr{\tilde{\mb X}\this}^T P^2 \mb y^{(k)} -\frac\mu2\tilde{\mb e}}.
\end{equation}

В силу соотношений
\begin{align}
	\cbr{\tilde {\mb X}\this}^T \tilde{\mb X}\this
		&= \tilde{\mb X}^T \tilde{\mb X} - \tilde{\mb x}_k \tilde{\mb x}_k^T, \\
	\cbr{\tilde {\mb X}\this}^T \mb y^{(k)}
		&= \tilde{\mb X}^T \mb y - y_k \tilde{\mb x}_k,
\end{align}
и матричного равенства Вудбери
\begin{equation}
	\inv{\mb A+\mb B \mb C} = \mb A^{-1} - \mb A^{-1} \mb B\inv{\mb I + \mb C \mb A^{-1} \mb B} \mb C \mb A^{-1}
\end{equation}
получаем

\def\A{\tilde{\mb X}^T\tilde{\mb X} + \beta \tilde{\mb I}_{\hat n}}
\def\xxt{\tilde{\mb x}_k \tilde{\mb x}_k^T}
\def\denominator{1 - \tilde{\mb x}_k^T\inv{\A}\tilde{\mb x}_k}
\begin{align}
	\besta\this
		&= \inv{\A + \cbr{- \xxt}}
		\cbr{\tilde{\mb X}^T \mb y - y_k\tilde{\mb x}_k -\frac\mu2\tilde{\mb e}} = \\
		&= \inv{\A} \cbr{\tilde{\mb X}^T P^2\mb y - y_k\tilde{\mb x}_k -\frac\mu2\tilde{\mb e}} - \\
		&- \frac{\inv{\A}\cbr{-\xxt}\inv{\A}\cbr{\tilde{\mb X}^T \mb y  -\frac\mu2\tilde{\mb e} - y_k\tilde{\mb x}_k}}{\denominator} = \\
		&= \besta + \frac{\inv{\A}\xxt\besta}{\denominator} - \\
		&- y_k\frac{\inv{\A}}{\denominator}\cbr{\tilde{\mb x}_k\cbr{\denominator} + \xxt\inv{\A}\tilde{\mb x}_k} = \\
		&= \besta + \frac{\inv{\A}\xxt\besta}{\denominator} - y_k\frac{\inv{\A}}{\denominator}\tilde{\mb x}_k.
\end{align}

Выразим результаты $\hat y\this_k = \tilde{\mb x}^T_k\besta\this$ классификации при исключенном $k$-м объекте через результат классификации $\hat y_k = \tilde{\mb x}^T_k\besta$ при использовании всей обучающей совокупности:

УБРАТЬ все Р - это настоящий ЛОО

\begin{align}
	\hat y\this_k
		&= \hat y_k + \hat y_k\frac{\tilde{\mb x}_k^T\inv{\A} \tilde{\mb x}_k}{\denominator}
		- y_k\frac{\tilde{\mb x}^T_k\inv{\A}\tilde{\mb x}_k}{\denominator} = \\
		&= \frac{\hat y_k}{\denominator} - y_k\frac{\tilde{\mb x}^T_k\inv{\A}\tilde{\mb x}_k}{\denominator}.
\end{align}

Тогда
\begin{align}
	\hat{\delta}_k\this
	&= y_k - \hat y\this_k = y_k - \frac{\hat y_k}{\denominator} + y_k\frac{\tilde{\mb x}^T_k\inv{\A}\tilde{\mb x}_k}{\denominator} = \\
	&= \frac{y_k - \hat y_k}{\denominator}.
\end{align}

Таким образом, теорема доказана.


\begin{theorem}
Про дифф кросс валидацию
\end{theorem}

\begin{align}
	\hat S(\lams)
	&= \frac1N\suml_{k=1}^N\hat\delta_{k, \lams}^2
	= \frac1N\suml_{k=1}^N\cbr{y_k - \tilde {\mb x}_k^T \besta}^2 = \\
	&= \frac1N\suml_{k=1}^N
		\cbr{y_k - \tilde {\mb x}_k^T
		\inv{\tilde{\mb X}^T_\lams P^2\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}}
		\cbr{\tilde{\mb X}^T_\lams P^2 y - \frac\mu2\tilde{\mb e}_\lams}
	}^2.
\end{align}

Положим $\mb \Delta = \mb P^2 - \mb I = \diag{p_1 - 1, \ldots, p_N - 1},$ тогда
\begin{align}
	&\dd{\hat\delta^2_k(\mb p \cond \lams)}{p_j}
	= 	2\delta_k(\mb p \cond \lams)\dd{\hat\delta_k(\mb p \cond \lams)}{p_j} = \\
	&= 	2\delta_k(\mb p \cond \lams)
		\dd{}{p_j}
		\fbr{
			\cbr{y_k - \tilde {\mb x}_k^T \inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
			\cbr{\tilde{\mb X}^T P^2 \mb y -\frac\mu2\tilde{\mb e}}}
		} = \\
	&= 2\delta_k(\mb p \cond \lams) \tilde {\mb x}_k^T
	\left\{
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
	  	\dd{\cbr{\tilde{\mb X}^T P^2 \mb y}}{p_j} -
	  	\right. \\
	&- 	\left.
	   	\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\dd{\cbr{\tilde{\mb X}^T P^2\tilde{\mb X}}}{p_j}
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\cbr{\tilde{\mb X}^T P^2 \mb y -\frac\mu2\tilde{\mb e}}
	\right\} = \\
	&= 2\delta_k(\mb p \cond \lams) \tilde {\mb x}_k^T
	\fbr{
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_{j,\lams}y_j
		-
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_j \tilde{\mb x}^T_j
		\besta
	}
\end{align}

\begin{align}
	\dd{\hat\delta^2_k(\mb p \cond \lams)}{p_k}
	&= 2\delta_k(\mb p \cond \lams) \tilde {\mb x}_k^T
	\fbr{
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_ky_k
		-
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_k \tilde{\mb x}^T_k
		\besta
	} = \\
	&= 2\delta_k(\mb p \cond \lams)
		\tilde {\mb x}_k^T
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_k \cbr{y_k - \hat y_k}
\end{align}

Изучим, как соотносятся
$\suml_{j=1}^N\sqr{\hat{\delta}_{j, \lams}^{(j)}}$
и $\suml_{j=1}^N \hat\delta_{j, \lams}^2$

\begin{align}
	\suml_{j=1}^N\sqr{\hat{\delta}_{j, \lams}^{(j)}} &=
		\suml_{j=1}^N\sqr{y_j - \mb x_j^T \hat {\mb a}_\lams^{(j)}} \\
	\suml_{j=1}^N \hat\delta_{j, \lams}^2 &=
		\suml_{j=1}^N \sqr{y_j - \mb x_j^T \hat {\mb a}_\lams}
\end{align}

% \end{proof}
% \newtheorem{о решении задачи оптимизации}{Решение $\hat a_{\lams}$ задачи оптимизации (\ref{LagrWEN}) является комбинацией решения }

\begin{align}
	% \dd{}{\mb p}\hat{S}_{\lambda_1, \lambda_2}&= \frac2N\cbr{y_j - x_j^T \dd{}{\mb p}\hat {\mb a}_{\lambda_1, \lambda_2}}.
	\dd{}{p_k}\hat{S}_{\lams}&= \frac2N\sum_{j=1}^N\cbr{y_j - x_j^T \dd{}{p_k}\hat {\mb a}_{\lams}}
\end{align}
