При поиске вектора $\hat{\mb a}$ мы получаем разбиение всех его компонент (а вместе с ними и признаков) на 3 группы: положительные, равные нулю и отрицательные. 
Пусть $I=\fbr{1,\ldots, n}$~--- множество всех признаков. Введём  обозначения:
\begin{align}
	\hat I^-_\lams &= \fbr{i\in I: \hat a_{i, \lams} < 0}, \\
	\hat I^0_\lams &= \fbr{i\in I: \hat a_{i, \lams} = 0}, \\
	\hat I^+_\lams &= \fbr{i\in I: \hat a_{i, \lams} > 0}.
\end{align}

Используя такие обозначения и тем самым зная знаки $\hat{\mb a}_\lams^i$, перепишем задачу (\ref{weighedENdelta}) в следующем виде:

\begin{align}
	% \label{min_idx}
	% J(\mb a \cond \lams)
	% 	&= \suml_{i\inotnull}\cbr{\beta a_i^2 + \mu \modul{a_i}}
	% 	+ \suml_{j=1}^N p_j\cbr{y_j - \suml_{i=1}^nx_{ij}a_i}^2 \to \min_{\mb a}, \\
	\notag
	J(\mb a, i\inotnull \cond \lams)
		&= \suml_{i\inotnull}\beta a_i^2 + \mu \suml_{i\in \hat I^+_\lams}a_i - \mu \suml_{i\in \hat I^-_\lams}a_i
		+ \suml_{j=1}^N p_j\cbr{y_j - \suml_{i\inotnull}x_{ij}a_i}^2 \to \\
		\label{knownA}
		&\to \min\cbr{a_i, i\inotnull}
\end{align}

Зная $\hat I^0_\lams,$ можно говорить о том, что $\hat{\mb a}$ --- решение является комбинацией решения $\tilde{{\mb a}}$ задачи (\ref{knownA}) и совокупности равенств $\mb a_i = 0, i\in \hat I^0_\lams$.

Введём обозначения, соответствующие только ненулевым компонентам $\hat{\mb a}$:
\begin{align*}
	\tilde{\mb a}_\lams &= (a_i, i\inotnull) \in \mathbb{R}^{\hat n_\lams}, \\
	\tilde{\mb x}_{j,\lams} &= (x_{ij}, i\inotnull) \in \mathbb{R}^{\hat n_\lams}, \\
	\tilde{\mb X}_\lams &= (\tilde{\mb x}_1, \ldots, \tilde{\mb x}_N)^T \in \mathbb{R}^{N \times \hat n_\lams}, \\
	\tilde{\mb e}_\lams &= (\tilde e_i, i\inotnull) \in \mathbb{R}^{\hat n_\lams},
		\tilde e_i = \begcas{+1, i \in \hat I^+_\lams \\-1, i \in \hat I^-_\lams}.
\end{align*}

При малых изменениях вектора весов $\mb p$ решение $\hat{\mb a}$ также изменится не сильно, а вместе с ним сохранятся и множества $\hat I^-_\lams, \hat I^0_\lams, \hat I^+_\lams.$ Поскольку при известном разбиении $I$ задача (\ref{knownA}) является квадратичной, то её решение может быть явно выписано.

Таким образом, в малой окрестности $\mb p$ будет выполнено следующее

\begin{statement}
\label{stmnt:locally_linear}
В малой окрестности $\mb p$ оптимальный вектор $\besta$ является решением системы линейных уравнений
\begin{equation*}
	\cbr{\tilde{\mb X}^T_\lams P\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}} \tilde{\mb a}
		= \tilde{\mb X}^T_\lams P y -\frac\mu2\tilde{\mb e}_\lams.
\end{equation*}
\end{statement}

\begin{Proof}
\begin{equation}
	\dd{}{\tilde a_i}J(\tilde{\mb a}, i\inotnull \cond \lams)
		= 2\beta \tilde a_i + \mu e_i - 2\suml_{j=1}^N p_j\cbr{y_j - \suml_{l\inotnull}x_{lj}\tilde a_l}x_{ij} = 0
\end{equation}
% Здесь есть дополнительный множитель, потерянный в публикации на кольт

Тем самым мы имеем систему линейных алгебраических уравнений на $a_i$
\begin{equation*}
	\beta \tilde a_i + \suml_{j=1}^N p_j\suml_{l\inotnull} x_{lj}x_{ij}\tilde a_l
		= \suml_{j=1}^N p_jy_jx_{ij} -\frac\mu2 \tilde e_i, i\inotnull.
\end{equation*}

В матричных обозначениях система принимает вид,
\begin{equation*}
	\cbr{\tilde{\mb X}^T_\lams P\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}} \tilde{\mb a}
		= \tilde{\mb X}^T_\lams P y -\frac\mu2\tilde{\mb e}_\lams,
\end{equation*}
а решение может быть выражено в виде
\begin{equation}
	\besta
		= \inv{\tilde{\mb X}^T_\lams P\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}}\cbr{\tilde{\mb X}^T_\lams P y -\frac\mu2\tilde{\mb e}_\lams}.
\end{equation}

Утверждение доказано.
\end{Proof}


\subsection{Оценка ошибки LOO}
Ранее в пункте (\ref{sub:intro:weight}) было показано, как LOO-кросс-валидацию можно описать в терминах взвешенных объектов.
Получим оценку скользящего контроля в следующей модели обучения. Предположим, что один раз обучившись при единичном векторе весов $\mb p = \mb e$ и найдя вектор $\hat{\mb a}(\mb e, \lams),$ мы получили множества $I^+, I^-, I^0,$ а затем будем поочерёдно присваивать объектам нулевой вес, при этом сохраняя фиксированными разбиение множества индексов. Тогда задача оптимизации будет оставаться квадратичной и её решение может быть проведено эффективнее, с использованием утверждения (\ref{stmnt:locally_linear}):
\begin{equation}
	S_{LOO} = \frac1N\suml_{k=1}^N \cbr{y_k - \mb x_k^T\hat{\mb a}(\beta, \mu, \mb e^{(i)})}^2.
\end{equation}
% \begin{align}
% 	\hat S(\lams) &= \frac1N\sum_{j=1}^N \hat\delta_{j, \lams}^2, \\
% 	\hat \delta_{j, \lams} &= y_j - \mb x_j^T \hat {\mb a}_\lams.
% \end{align}
% В используемых обозначениях кросс-валидацию можно рассматривать как обучение с поочерёдным присвоением объектам нулевых весов ($p_j=0$).
% Усреднённые квадратичные остатки

% \begin{align}
% 	\hat{S}_{\text{LOO}}(\lams)&=\frac1N\sum_{j=1}^N\sqr{\hat{\delta}_{j, \lams}^{(j)}}, \\
% 	\hat \delta_{j, \lams}^{(j)} &= y_j - \mb x_j^T \hat {\mb a}_\lams^{(j)}.
% \end{align}

% $\hat {\mb a}_{\lams}^{(j)}$~--- результат обучения при векторе весов $p^{(j)}, p_i^{(j)} = \sbr{i \ne j}$ (единицы на всех позициях, кроме $j$-ой), т.е. при исключённом объекте $j.$

% \begin{equation}
% 	\hat \delta_{j, \lams}^{(j)}
% 		= y_j - \mb x_j^T \hat {\mb a}_\lams^{(j)}
% 		= y_j - \suml_{i\inotnull} x_{ji} \hat{\mb a}_{i, \lams}^{(j)}
% 		= y_j - \tilde{\mb x}_j^T \besta_\lams^{(j)}.
% \end{equation}

\begin{statement}
При фиксированном разбиении множества индексов оценка скользящего контроля может быть вычислена в виде
\begin{equation*}
	S_{LOO, I^+, I^-} = \frac1N\suml_{k=1}^N\frac{y_k - \mb x_k^T\hat{\mb a}(\lams, \mb e)}{\denominator}.
\end{equation*}
\end{statement}

\begin{Proof}
\def\this{^{(k)}}
В течение доказательства опустим индексы $\lams$ для упрощения обозначений. 
Верхний индекс ${}\this$ будем добавлять объектам, которые построены по всем объектам обучающей совокупности, кроме $\omega_k$, аналогично добавлению $\tilde{\:}$ над матрицами и векторами, из которых исключены компоненты, соответствующие нулевым элементам вектора $\hat{\mb a}.$

Согласно утверждению (\ref{stmnt:locally_linear})

\begin{equation*}
	\besta^{(k)}
		= \cbr{\cbr{\tilde{\mb X}\this}^T \tilde{\mb X}\this + \beta \tilde{\mb I}_{\hat n_\lams}}^{-1}\cbr{\cbr{\tilde{\mb X}\this}^T \mb y^{(k)} -\frac\mu2\tilde{\mb e}}.
\end{equation*}

Используя соотношения
\begin{align*}
	\cbr{\tilde {\mb X}\this}^T \tilde{\mb X}\this
		&= \tilde{\mb X}^T \tilde{\mb X} - \tilde{\mb x}_k \tilde{\mb x}_k^T, \\
	\cbr{\tilde {\mb X}\this}^T \mb y^{(k)}
		&= \tilde{\mb X}^T \mb y - y_k \tilde{\mb x}_k
\end{align*}
и матричное равенство Вудбери
\begin{equation*}
	\inv{\mb A+\mb B \mb C} = \mb A^{-1} - \mb A^{-1} \mb B\inv{\mb I + \mb C \mb A^{-1} \mb B} \mb C \mb A^{-1},
\end{equation*}

получаем
\begin{align*}
	\besta\this
		&= \inv{\A + \cbr{- \xxt}}
		\cbr{\tilde{\mb X}^T \mb y - y_k\tilde{\mb x}_k -\frac\mu2\tilde{\mb e}} = \\
		&= \inv{\A} \cbr{\tilde{\mb X}^T P^2\mb y - y_k\tilde{\mb x}_k -\frac\mu2\tilde{\mb e}} - \\
		&- \frac{\inv{\A}\cbr{-\xxt}\inv{\A}\cbr{\tilde{\mb X}^T \mb y  -\frac\mu2\tilde{\mb e} - y_k\tilde{\mb x}_k}}{\denominator} = \\
		&= \besta + \frac{\inv{\A}\xxt\besta}{\denominator}
		- y_k\frac{\inv{\A}}{\denominator} \cdot \\
		&\cdot \fbr{\tilde{\mb x}_k\cbr{\denominator} + \xxt\inv{\A}\tilde{\mb x}_k} = \\
		&= \besta + \frac{\inv{\A}\xxt\besta}{\denominator} - y_k\frac{\inv{\A}}{\denominator}\tilde{\mb x}_k.
\end{align*}

Выразим результаты $\hat y\this_k = \tilde{\mb x}^T_k\besta\this$ классификации при исключенном $k$-м объекте через результат классификации $\hat y_k = \tilde{\mb x}^T_k\besta$ при использовании всей обучающей совокупности:

% УБРАТЬ все Р - это настоящий ЛОО

\begin{align*}
	\hat y\this_k
		&= \hat y_k + \hat y_k\frac{\tilde{\mb x}_k^T\inv{\A} \tilde{\mb x}_k}{\denominator}
		- y_k\frac{\tilde{\mb x}^T_k\inv{\A}\tilde{\mb x}_k}{\denominator} = \\
		&= \frac{\hat y_k}{\denominator} - y_k\frac{\tilde{\mb x}^T_k\inv{\A}\tilde{\mb x}_k}{\denominator}.
\end{align*}

Тогда регрессионные остатки окажутся равными
\begin{align*}
	\hat{\delta}_k\this
	&= y_k - \hat y\this_k = y_k - \frac{\hat y_k}{\denominator} + y_k\frac{\tilde{\mb x}^T_k\inv{\A}\tilde{\mb x}_k}{\denominator} = \\
	&= \frac{y_k - \hat y_k}{\denominator}.
\end{align*}

Доказательство завершено.
\end{Proof}

\begin{theorem}
Про дифф кросс валидацию
\end{theorem}

\begin{align}
	\hat S(\lams)
	&= \frac1N\suml_{k=1}^N\hat\delta_{k, \lams}^2
	= \frac1N\suml_{k=1}^N\cbr{y_k - \tilde {\mb x}_k^T \besta}^2 = \\
	&= \frac1N\suml_{k=1}^N
		\cbr{y_k - \tilde {\mb x}_k^T
		\inv{\tilde{\mb X}^T_\lams P^2\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}}
		\cbr{\tilde{\mb X}^T_\lams P^2 y - \frac\mu2\tilde{\mb e}_\lams}
	}^2.
\end{align}

Положим $\mb \Delta = \mb P^2 - \mb I = \diag{p_1 - 1, \ldots, p_N - 1},$ тогда
\begin{align}
	\dd{\hat\delta_k(\mb p \cond \lams)}{p_j}
	&= 	\dd{}{p_j}
		\fbr{
			y_k - \tilde {\mb x}_k^T \inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
			\cbr{\tilde{\mb X}^T P^2 \mb y -\frac\mu2\tilde{\mb e}}
		} = \\
	&= - \tilde {\mb x}_k^T
	\left\{
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
	  	\dd{\cbr{\tilde{\mb X}^T P^2 \mb y}}{p_j} -
	  	\right. \\
	&- 	\left.
	   	\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\dd{\cbr{\tilde{\mb X}^T P^2\tilde{\mb X}}}{p_j}
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\cbr{\tilde{\mb X}^T P^2 \mb y -\frac\mu2\tilde{\mb e}}
	\right\} = \\
	&= - \tilde {\mb x}_k^T
	\fbr{
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_jy_j
		-
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_j \tilde{\mb x}^T_j
		\besta
	}
\end{align}

\begin{align}
	\dd{\hat\delta_k}{p_k}
	&= - \tilde {\mb x}_k^T
	\fbr{
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_ky_k
		-
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_k \tilde{\mb x}^T_k
		\besta
	} = \\
	&= -\tilde {\mb x}_k^T
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_k \cbr{y_k - \hat y_k}
	= - \tilde {\mb x}_k^T
		\inv{\tilde{\mb X}^T P^2\tilde{\mb X} + \beta I_{\hat n}}
		\tilde{\mb x}_k \hat \delta_k.
\end{align}

Тогда
\begin{align}
	\left.\fbr{
		\hat S(\mb p, \lams)
	- 	\suml_{i=1}^N\dd{\delta_i^2(\mb p, \lams)}{p_i}
	}\right|_{\mb p=\mb e}
	&= 	\frac1N\suml_{i=1}^N
		\cbr{
			\hat \delta_{i, \lams}^2
		+ 	2 \delta_i(\mb p, \lams)
			\tilde {\mb x}_i^T
			\inv{\tilde{\mb X}^T \tilde{\mb X} + \beta I_{\hat n}}
			\tilde{\mb x}_i \hat \delta_i
		} = \\
	&= 	\frac1N\suml_{i=1}^N
			\hat \delta_{i, \lams}^2
		\cbr{
			1
		+ 	2
			\tilde {\mb x}_i^T
			\inv{\tilde{\mb X}^T \tilde{\mb X} + \beta I_{\hat n}}
			\tilde{\mb x}_i
		}
\end{align}

Изучим, как соотносятся
$\suml_{j=1}^N\sqr{\hat{\delta}_{j, \lams}^{(j)}}$
и $\suml_{j=1}^N \hat\delta_{j, \lams}^2$

\begin{align}
	\suml_{j=1}^N\sqr{\hat{\delta}_{j, \lams}^{(j)}} &=
		\suml_{j=1}^N\sqr{y_j - \mb x_j^T \hat {\mb a}_\lams^{(j)}} \\
	\suml_{j=1}^N \hat\delta_{j, \lams}^2 &=
		\suml_{j=1}^N \sqr{y_j - \mb x_j^T \hat {\mb a}_\lams}
\end{align}

% \end{proof}
% \newtheorem{о решении задачи оптимизации}{Решение $\hat a_{\lams}$ задачи оптимизации (\ref{LagrWEN}) является комбинацией решения }

\begin{align}
	% \dd{}{\mb p}\hat{S}_{\lambda_1, \lambda_2}&= \frac2N\cbr{y_j - x_j^T \dd{}{\mb p}\hat {\mb a}_{\lambda_1, \lambda_2}}.
	\dd{}{p_k}\hat{S}_{\lams}&= \frac2N\sum_{j=1}^N\cbr{y_j - x_j^T \dd{}{p_k}\hat {\mb a}_{\lams}}
\end{align}
