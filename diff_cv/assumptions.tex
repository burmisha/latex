Рассматривается задача восстановления числовой регрессии по обучающей выборке объектов, задынных своим признаковым описанием. 
Пусть $\Omega$~--- некоторое множество объектов реального мира, каждому элементу $\omega$ которого сопоставлено число $y(\omega)\in \mathbb Y \subset \mathbb R,$ 
а каждый объект $\omega \in \Omega$ представлен конечным множеством своих числовых признаков: $\mb x(\omega) = \cbr{x_1, \ldots, x_n}^T \in \mathbb X \subset \mathbb R^n.$
То есть предположим существование функций $y\colon \Omega \to \mathbb R$ и $\mb x \colon \Omega \to \mathbb R^n.$

Пусть наблюдателю известны значения этих функций в пределах некоторой конечной обучающей совокупности $\Omega^* = \fbr{\omega_j \cond j = 1,\ldots, N}$: $D = \fbr{\mb x(\omega), y(\omega) \cond \omega\in\Omega^*}.$
Ставится задача построить функцию $\hat y: \mathbb X \to \mathbb Y$ по известному $D$ такую, 
что функция $\mb x \circ \hat y \colon \Omega \to \mathbb Y$ будет как можно точнее описывать функцию $y.$

Такая постановка задачи является весьма общей, и для получения конкретного решения требуется введение дополнительных ограничений. 
Будем искать решение в классе линейных функций, параметризованных следующим образом:
\begin{equation*}
	\hat y(\mb x) = \mb a^T \mb x + b, 
	\:\text{где $\mb a = (a_1, \ldots, a_n)^T \in \mathbb R^n$ и $b\in \mathbb R.$}
\end{equation*}

Введём обозначения:
\begin{align*}
	\mb x(\omega_j)	&= \mb x_j, j=1, \ldots, N, \\
	y(\omega_j) 	&= y_j, j=1, \ldots, N, \\
	\hat y(x_j) 	&= \hat y_j, j=1, \ldots, N.
\end{align*}

В качестве меры близости функций $\mb x \circ \hat y$ и $y$ предлагается использовать средний квадрат их разницы на обучающей совокупности:
\begin{equation*}
	\frac1N\suml_{j=1}^N \cbr{y_j - \hat y_j}^2.
\end{equation*}

Тогда мы имеем следующую задачу оптимизации 
\begin{equation*}
	\cbr{\hat{\mb a}, \hat b} = \argmin \suml_{j=1}^N\cbr{y_j - \mb a^T \mb x_j - b}^2, 
\end{equation*}
однако такая постановка задачи может вызвать трудности при непосредственном поиске оптимальных параметров $\cbr{\hat{\mb a}, \hat b}$ в случае мультиколлинеарности используемых признаков, 
выражающееся в численной неустойчивости ответа и невозможности найти единственный оптимум, т.е. задача оказывается нерегулярной.

С целью регуляризации задачи в [] предлагается ввести штраф 
\begin{equation*}
	\beta \norm{\mb a}_{\mathbb R^2}^2 + \mu \norm{\mb a}_{\mathbb R} 
	= \beta \suml_{i=1}^n a_i^2 + \mu \suml_{i=1}^n \abs{a_i}.
\end{equation*}
