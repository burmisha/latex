\subsection{Постановка задачи}
Рассматривается задача восстановления числовой регрессии по обучающей выборке объектов, задынных своим признаковым описанием. 
Пусть $\Omega$~--- некоторое множество объектов реального мира, каждому элементу $\omega$ которого сопоставлено число $y(\omega)\in \mathbb Y \subset \mathbb R,$ 
а каждый объект $\omega \in \Omega$ представлен конечным множеством своих числовых признаков: $\mb x(\omega) = \cbr{x_1, \ldots, x_n}^T \in \mathbb X \subset \mathbb R^n.$
То есть предположим существование функций $y\colon \Omega \to \mathbb R$ и $\mb x \colon \Omega \to \mathbb R^n.$

Пусть наблюдателю известны значения этих функций в пределах некоторой конечной обучающей совокупности $\Omega^* = \fbr{\omega_j \cond j = 1,\ldots, N}$: $D = \fbr{\mb x(\omega), y(\omega) \cond \omega\in\Omega^*}.$
Ставится задача построить функцию $\hat y: \mathbb X \to \mathbb Y$ по известному $D$ такую, 
что функция $\mb x \circ \hat y \colon \Omega \to \mathbb Y$ будет как можно точнее описывать функцию $y.$

\subsection{Задача линейной регрессии}
Такая постановка задачи является весьма общей, и для получения конкретного решения требуется введение дополнительных ограничений. 
Будем искать решение в классе линейных функций, параметризованных следующим образом:
\begin{equation*}
	\hat y(\mb x) = \mb a^T \mb x + b, 
	\:\text{где $\mb a = (a_1, \ldots, a_n)^T \in \mathbb R^n$ и $b\in \mathbb R.$}
\end{equation*}

Введём обозначения:
\begin{align*}
	\mb x(\omega_j)	&= \mb x_j, j=1, \ldots, N  \text{ --- векторы признаков объектов,}\\
	\mb X 			&= \norm{x_{ij}}_{i=1, j=1}^{n,N} = \cbr{\mb x_1, \ldots, \mb x_N}^T \text{ --- матрица объекты-признаки,} \\
	y(\omega_j) 	&= y_j, j=1, \ldots, N, \\
	\hat y(x_j) 	&= \hat y_j, j=1, \ldots, N, \\
\end{align*}

В качестве меры близости функций $\mb x \circ \hat y$ и $y$ предлагается использовать средний квадрат их разницы на обучающей совокупности:
\begin{equation*}
	\frac1N\suml_{j=1}^N \cbr{y_j - \hat y_j}^2.
\end{equation*}

Тогда мы имеем следующую задачу оптимизации 
\begin{equation}
	\label{bestAB}
	\cbr{\hat{\mb a}, \hat b} = \argmin \suml_{j=1}^N\cbr{y_j - \mb a^T \mb x_j - b}^2, 
\end{equation}
однако такая постановка задачи может вызвать трудности при непосредственном поиске оптимальных параметров $\cbr{\hat{\mb a}, \hat b}$ в случае мультиколлинеарности используемых признаков, 
выражающееся в численной неустойчивости ответа и невозможности найти единственный оптимум, т.е. задача оказывается нерегулярной.

\subsection{Регуляризация Elastic Net}
С целью регуляризации задачи (\ref{bestAB}) в [] предлагается ввести штраф, называемый регуляризацией Elastic Net:
\begin{equation}
	\label{ENregularization}
	\beta \norm{\mb a}_{\mathbb R^2}^2 + \mu \norm{\mb a}_{\mathbb R} 
	= \beta \suml_{i=1}^n a_i^2 + \mu \suml_{i=1}^n \abs{a_i}.
\end{equation}

Что приводит к задаче оптимизации: 
\begin{equation}
	\label{basicEN}
	\sum_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i}} 
	+ \sum_{j=1}^N\cbr{y_j - \mb a^T \mb x_j - b}^2 
	\to \min_{\mb a, b}
\end{equation}
Эта задача не является квадратичной при $\mu > 0,$ и её решение мы обсудим позднее.

Отметим, что этот подход обобщает в себе регуляризации, используемые в методе лассо (LASSO, []) и методе опорных вектором (SVM) в задачах регрессии ([]).

Регуляризация (\ref{ENregularization}) в равной степени штрафует все компоненты вектора $\mb a\in \mathbb R^n,$ 
поэтому необходимо убедиться в том, что все признаки нормированы. 
Этого можно добиться линейным преобразованием признаков. 
Одновременно с этим предлагается центрировать признаки, чтобы избавиться от параметра $b$.

Тогда по обучающей совокупности $D$ линейным преобразованием построим совокупность
\begin{equation*}
	D^* = \fbr{\mb x^*_i, y^*_i \cond i = 1, \ldots, N}, 
\end{equation*}
удовлетворяющую следующим условиям:
\begin{align}
	\label{normalization-start}
	\mb 0 	&= \frac1N\suml_{i=1}^N\mb x^*_i, \\
	0 		&= \frac1N\suml_{i=1}^N y^*_i, \\
	\label{normalization-end}
	1 		&= \frac1N\suml_{i=1}^N\cbr{\mb x^*_{ij}}^2, j=1,\ldots, n.
\end{align}

Подходящее линейное преобразование задаётся соотношениями:
\begin{align*}
	x^*_{ij} 	&= \frac{x_{ij} - \bar x_j}{d_j}, \oi iN, \oi jn;\\
	\bar x_j 	&= \frac1N\suml_{i=1}^N x_{ij}, \oi jn;\\
	d_j 		&= \sqrt{\frac1N\suml_{i=1}^N \cbr{x_{ij}-\bar x_j}^2}, \oi jn;\\
	y^*_i 		&= y_i - \bar y, \oi iN;\\
	\bar y 		&= \frac1N\suml_{i=1}^N y_i. \\
\end{align*}

В дальнейшем для упрощения изложения мы будем везде считать, что это преобразование уже произведено, и условия (\ref{normalization-start}--\ref{normalization-end}) выполнены для совокупности $D.$

В таком случае задача (\ref{basicEN}) принимает вид
\begin{equation}
	\label{mainEN}
	\sum_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i}} 
	+ \sum_{j=1}^N\cbr{y_j - \mb a^T \mb x_j}^2 
	\to \min_{\mb a}.
\end{equation}

\subsection{Скользящий контроль для поиска оптимальных параметров регуляризации} % (fold)
\label{sub:intro:LOO}
Для подбора оптимальных значений параметров $\beta, \mu$ предлагается воспользоваться критерием скользящего контроля leave-one-out (LOO, []).
Для этого построим совокупность множеств обучения $D^{(k)}, \oi kN$ из $D$ путем поочередного исключения из него каждого из объектов: 
\begin{equation}
	D^{(k)}=\fbr{(\mb x_i, y_i) \cond \oi iN, i\ne k}.
\end{equation}
Для каждого полученного обучающего множества решим задачу (\ref{mainEN}) и получим оптимальный вектор 
\begin{equation*}
	\hat{\mb a}^{(k)} 
	= \argmin \suml_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i}} 
	+ \suml_{\oi jN, j\ne k}\cbr{y_j - \mb a^T \mb x_j}^2.
\end{equation*}

Критерием качества будет являться среднее ошибок алгоритмов на объектах исключенных из исходного множества обучения:
\begin{equation}
	Q(\beta, \mu) = \frac1N\suml_{k=1}^N \cbr{y_k - \mb x_k^T\hat{\mb a}^{(k)}}^2.
\end{equation}

Таким образом критерий традиционной кросс-валидации принимает вид:
\begin{equation}
	\label{LOOcriteria}
	\cbr{\hat \beta, \hat \mu} = \argmin \frac1N\suml_{k=1}^N \cbr{y_k - \mb x_k^T\hat{\mb a}^{(k)}}^2.
\end{equation}

В такой форме явно видно, что получение оценки $Q(\beta, \mu)$ требует построение всех векторов $\hat{\mb a}^{(k)}, \oi kN,$ то есть придётся обучаться $N$ раз. 
% subsection subsection_name (end)

% \subsection{Скользящий контроль для поиска оптимальных параметров регуляризации}

\subsection{Взвешивание объектов обучающей совокупности}
\label{sub:intro:weight}
В работе [] была предложена идея дифференциального скользящего контроля. 
Она заключается в присвоении каждому объекту $\omega_j, \oi jN$ из обучающей совокупности некоторого числа $p_j, \oi jN,$ которое означает вес объекта при обучении. 
Задача оптимизации (\ref{mainEN}) при этом принимает вид: 
\begin{equation}
	\label{weighedEN}
	\sum_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i}} 
	+ \sum_{j=1}^Np_j\cbr{y_j - \mb a^T \mb x_j}^2 
	\to \min_{\mb a, b}.
\end{equation}

Теперь оптимальный вектор $\hat{\mb a}$ зависит не только от параметров $\mu, \beta$, но и от вектора весов $\mb p = (p_1, \ldots, p_N).$
Тогда критерий (\ref{LOOcriteria}) можно переписать в виде:
\begin{align*}
	\cbr{\hat \beta, \hat \mu} &= \argmin \frac1N\suml_{k=1}^N \cbr{y_k - \mb x_k^T\hat{\mb a}(\beta, \mu, \mb e^{(i)})}^2,\\
	\text{ где } \mb e^{(i)} &= (e^{(i)}_1, \ldots, e^{(i)}_N)^T, e^{(i)}_k=\begcas{1, i\ne k, \\ 0, i = k.}
\end{align*}

Задача в форме (\ref{weighedEN}) обобщает ранее рассмотренные (\ref{basicEN}, \ref{mainEN}), поэтому далее мы будем решать именно её.
