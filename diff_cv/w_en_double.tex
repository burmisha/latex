Рассмотрим задачу (\ref{weighedEN}) и перепишем её в виде
\begin{equation}
	\label{weighedENdelta}
	\begin{cases}
		\suml_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i}} 
	+ \suml_{j=1}^N p_j\delta_j^2 \to \min_{\mb a}, \\
	\delta_j = y_j - \suml_{i=1}^nx_{ij}a_i, j=1,\ldots,N,
	\end{cases}
\end{equation}
где $\delta = \cbr{\delta_1, \ldots, \delta_N}^T$ --- вектор регрессионных остатков.

Функция Лагранжа (здесь $\mb \lambda = \cbr{\idots \lambda iN}$ --- вектор двойственных переменных):
\begin{equation}
	\label{LagrWEN}
	\begin{split}
		L&(\mb a, \mb \delta, \mb \lambda) 
	= \suml_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i}} + \suml_{j=1}^N p_j\delta_j^2 - \sum_{j=1}^N\lambda_j\cbr{\delta_j - y_j + \suml_{i=1}^nx_{ij}a_i} = \\
	&= \suml_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i} - \suml_{j=1}^N\lambda_jx_{ij}a_i} + \suml_{j=1}^N \cbr{p_j\delta_j^2  - \lambda_j\cbr{\delta_j - y_j}} \to 
	\begin{cases}
		\min_{\mb a, \mb \delta}\\
		\max_{\mb \lambda}\\
	\end{cases}
	\end{split}
\end{equation}

Положим 
\begin{equation}
	\label{partialLagr}
	\LiPart = \beta a_i^2 + \mu \modul{a_i} - \cbr{\sumLambdaX}a_i \to \min_{a_i}.
\end{equation}

Каждая из функций $L_i$ является кусочно заданной:
\begin{equation}
	\label{partialLagrParted}
	\begin{split}
		\LiPart 
		&= \begin{cases}
			\beta a_i^2 + \mu a_i - \cbr{\sumLambdaX}a_i, &a_i > 0,\\
			0, &a_i = 0,\\
			\beta a_i^2 - \mu a_i - \cbr{\sumLambdaX}a_i, &a_i < 0,\\
		\end{cases} \quad= \\
		&= \begin{cases}
			\beta a_i^2 - \cbr{\sumLambdaX-\mu}a_i, &a_i > 0,\\
			0, &a_i = 0,\\
			\beta a_i^2 - \cbr{\sumLambdaX+\mu}a_i, &a_i < 0.\\
		\end{cases}	
	\end{split}
\end{equation}

Исходная постановка задачи (\ref{basicEN}) предполагает, что $\beta\ge0.$ Изучим случай $\beta > 0,$ получив в (\ref{partialLagrParted}) квадратные трехчлены:
\begin{equation}
	\label{hatAi}
	\begin{split}
		\hat a_i &= \argmin_{a_i} \LiPart = \\
				 &=	\argmin_{a_i} \fbr{
				 	\begin{aligned}
						\beta a_i^2 - \cbr{\sumLambdaX-\mu}a_i, &a_i > 0,\\
						0, \qquad\qquad &a_i = 0,\\
						\beta a_i^2 - \cbr{\sumLambdaX+\mu}a_i, &a_i < 0.\\
					\end{aligned}	
					}
	\end{split}
\end{equation}

Для поиска точки минимума в (\ref{hatAi}) рассмотрим 3 случая:
\begin{align}
	\label{hatAicases-start}
	         &\sumLambdaX \le -\mu 	&\Rightarrow \hat a_i &= \frac{\sumLambdaX+\mu}{2\beta}, \\
	-\mu <   &\sumLambdaX < \mu 	&\Rightarrow \hat a_i &= 0, \\
	\label{hatAicases-end}
	\mu \le &\sumLambdaX 			&\Rightarrow \hat a_i &= \frac{\sumLambdaX-\mu}{2\beta}.
\end{align}

% a x^2 + b x + c = a(x-b/{2a})^2 + c - b^2/{4a} = a(x-x_0)^2 + c - a x_0^2

Подставляя значения из (\ref{hatAicases-start}--\ref{hatAicases-end}) в (\ref{partialLagrParted}), получим решение задачи (\ref{partialLagrParted}) минимизации по $a_i$:
\begin{align}
	\label{hatLiSquared}
	\begin{split}
		\hat L_i(\mb \lambda) &= \min_{a_i}L_i(a_i, \mb \lambda) =
		\begin{cases}
			-\cfrac1{4\beta}\cbr{\sumLambdaX+\mu}^2, &\sumLambdaX \le -\mu,\\
			0, 										&-\mu < \sumLambdaX < \mu,\\
			-\cfrac1{4\beta}\cbr{\sumLambdaX-\mu}^2, &\mu\le\sumLambdaX,
		\end{cases} = \\
		&= -\frac1{4\beta}
		\begin{cases}
			\cbr{-\sumLambdaX-\mu}^2, &\sumLambdaX \le -\mu,\\
			0, 	 					&-\mu < \sumLambdaX < \mu,\\
			\cbr{\sumLambdaX-\mu}^2, &\mu\le\sumLambdaX.
		\end{cases} 	\\
		&= -\frac1{4\beta} \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu-\sumLambdaX}}^2.
	\end{split}
\end{align}
% (Здесь не сходится с результатами doc-файла)

Теперь перейдём к минимизации по $\cbr{\delta_1, \ldots, \delta_N}:$
\begin{align}
	\notag
	0 &= \dd{}{\delta_j}L(\mb a,\mb \delta, \mb \lambda) = 2p_j\delta_j - \lambda_j, \\
	\label{deltaJ}
	\delta_j &= \frac{\lambda_j}{2p_j}.
\end{align}

Подставим результаты (\ref{hatLiSquared}, \ref{deltaJ}) в (\ref{LagrWEN}):
\begin{align*} 
	\hat L&\cbr{\mb a,\mb \delta, \mb \lambda} 
		= \suml_{i=1}^n \hat L_i(a_i,\mb \lambda)
		+ \suml_{j=1}^N \cbr{p_j\delta_j^2  - \lambda_j\cbr{\delta_j - y_j}} = \\
		&= -\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu-\sumLambdaX}}^2
		+ \suml_{j=1}^N \cbr{p_j\cbr{\frac{\lambda_j}{2p_j}}^2  - \lambda_j\cbr{\frac{\lambda_j}{2p_j} - y_j}} = \\
		&= -\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu - \sumLambdaX}}^2
		- \suml_{j=1}^N \cbr{ \frac{\lambda_j^2}{4p_j}  - \lambda_j y_j}.
\end{align*}

Таким образом, задача (\ref{LagrWEN}) свелась к 
\begin{align}
-\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu - \sumLambdaX}}^2
		- \suml_{j=1}^N \cbr{ \frac{\lambda_j^2}{4p_j}  - \lambda_j y_j} \to \max_{\mb \lambda}.
\end{align}

Используя соотношение (\ref{deltaJ}), получим эквивалентную задачу:
\begin{align*} 
	% \hat L&\cbr{\mb a,\mb \delta, \mb \lambda} 
	% 	= \suml_{i=1}^n \hat L_i(a_i,\mb \lambda)
	% 	+ \suml_{j=1}^N % \cbr{p_j\cbr{\frac{\lambda_j}{2p_j}}^2  - \lambda_j\cbr{\frac{\lambda_j}{2p_j} - y_j}} = \\
	% 		\cbr{p_j\delta_j^2  - \lambda_j\cbr{\delta_j - y_j}} = \\
	% 	&= -\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu-\sumLambdaX}}^2
	% 	- \suml_{j=1}^N \cbr{p_j\delta_j^2  -  2 \delta_j p_j (\delta_j - y_j)} = \\
	% 	&= 
	-\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \suml_{j=1}^N2 \delta_j p_jx_{ij}, 0, \mu-\suml_{j=1}^N2 \delta_j p_jx_{ij}}}^2
		- \suml_{j=1}^N \cbr{ \delta_j^2 p_j  - 2\delta_j p_j y_j} \to \max_{\mb \delta}.
\end{align*}
Отметим, что задача осталась задачей максимизации.

Положим 
\begin{align*}
	W(\mb \delta) 
	= \frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \suml_{j=1}^N2 \delta_j p_jx_{ij}, 0, \mu-\suml_{j=1}^N2 \delta_j p_jx_{ij}}}^2
		+ \suml_{j=1}^N \cbr{ \delta_j^2 p_j  - 2\delta_j p_j y_j}.
\end{align*}

В матричных обозначениях получаем:
\begin{align*}
	&W(\mb \delta) = \frac1{2\beta} \suml_{i=1}^n \cbr{\min\fbr{\frac\mu2 + \mb \delta^T P\mb x_i, 0, \frac\mu2- \mb \delta^T P\mb x_i}}^2
		+ (\mb \delta - \mb y)^T P (\mb \delta - \mb y) \to \min_{\mb \delta}, \\
	&\text{где } P = P^T= \diag{\idots p1N} \in \mathbb R^{N\times N}.
\end{align*}

Итак, достаточно решить задачу 
\begin{equation}
	\label{easyOpt}
	W(\mb \delta) \to \min_{\mb \delta}.
\end{equation}

Пусть $\hat{\mb \delta}$~--- решение задачи (\ref{easyOpt}). Оно единственно в силу выпуклости функции $W(\mb \delta)$ по $\mb \delta$ и может быть найдено методами вычислительной оптимизации.
Тогда оптимальное $\hat{\mb a}$ можно определить из соотношений (\ref{deltaJ}) и (\ref{hatAicases-start}--\ref{hatAicases-end}).

\subsection{Зависимость решения от весов объектов из обучения}

При поиске вектора $\hat{\mb a}$ мы получаем разбиение всех его компонент (а вместе с ними и признаков) на 3 группы: положительные, равные нулю и отрицательные. 
Пусть $I=\fbr{1,\ldots, n}$~--- множество всех признаков. Введём  обозначения:
\begin{align}
	\hat I^-_\lams &= \fbr{i\in I: \hat a_{i, \lams} < 0}, \\
	\hat I^0_\lams &= \fbr{i\in I: \hat a_{i, \lams} = 0}, \\
	\hat I^+_\lams &= \fbr{i\in I: \hat a_{i, \lams} > 0}.
\end{align}

Используя такие обозначения и тем самым зная знаки $\hat{\mb a}_\lams^i$, перепишем задачу (\ref{weighedENdelta}) в следующем виде:

\begin{align}
	% \label{min_idx}
	% J(\mb a \cond \lams)
	% 	&= \suml_{i\inotnull}\cbr{\beta a_i^2 + \mu \modul{a_i}}
	% 	+ \suml_{j=1}^N p_j\cbr{y_j - \suml_{i=1}^nx_{ij}a_i}^2 \to \min_{\mb a}, \\
	\notag
	J(\mb a, i\inotnull \cond \lams)
		&= \suml_{i\inotnull}\beta a_i^2 + \mu \suml_{i\in \hat I^+_\lams}a_i - \mu \suml_{i\in \hat I^-_\lams}a_i
		+ \suml_{j=1}^N p_j\cbr{y_j - \suml_{i\inotnull}x_{ij}a_i}^2 \to \\
		\label{knownA}
		&\to \min\cbr{a_i, i\inotnull}
\end{align}

Зная $\hat I^0_\lams,$ можно говорить о том, что $\hat{\mb a}$ --- решение является комбинацией решения $\tilde{{\mb a}}$ задачи (\ref{knownA}) и совокупности равенств $\mb a_i = 0, i\in \hat I^0_\lams$.

Введём обозначения, соответствующие только ненулевым компонентам $\hat{\mb a}$:
\begin{align*}
	\tilde{\mb a}_\lams &= (a_i, i\inotnull) \in \mathbb{R}^{\hat n_\lams}, \\
	\tilde{\mb x}_{j,\lams} &= (x_{ij}, i\inotnull) \in \mathbb{R}^{\hat n_\lams}, \\
	\tilde{\mb X}_\lams &= (\tilde{\mb x}_1, \ldots, \tilde{\mb x}_N)^T \in \mathbb{R}^{N \times \hat n_\lams}, \\
	\tilde{\mb e}_\lams &= (\tilde e_i, i\inotnull) \in \mathbb{R}^{\hat n_\lams},
		\tilde e_i = \begcas{+1, i \in \hat I^+_\lams \\-1, i \in \hat I^-_\lams}.
\end{align*}

При малых изменениях вектора весов $\mb p$ решение $\hat{\mb a}$ также изменится не сильно, а вместе с ним сохранятся и множества $\hat I^-_\lams, \hat I^0_\lams, \hat I^+_\lams.$ Поскольку при известном разбиении $I$ задача (\ref{knownA}) является квадратичной, то её решение может быть явно выписано.

Таким образом, в малой окрестности $\mb p$ будет выполнено следующее

\begin{statement}
\label{stmnt:locally_linear}
В малой окрестности $\mb p$ оптимальный вектор $\besta$ является решением системы линейных уравнений
\begin{equation*}
	\cbr{\tilde{\mb X}^T_\lams P\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}} \tilde{\mb a}
		= \tilde{\mb X}^T_\lams P y -\frac\mu2\tilde{\mb e}_\lams.
\end{equation*}
\end{statement}

\begin{Proof}
\begin{equation}
	\dd{}{\tilde a_i}J(\tilde{\mb a}, i\inotnull \cond \lams)
		= 2\beta \tilde a_i + \mu e_i - 2\suml_{j=1}^N p_j\cbr{y_j - \suml_{l\inotnull}x_{lj}\tilde a_l}x_{ij} = 0
\end{equation}
% Здесь есть дополнительный множитель, потерянный в публикации на кольт

Тем самым мы имеем систему линейных алгебраических уравнений на $a_i$
\begin{equation*}
	\beta \tilde a_i + \suml_{j=1}^N p_j\suml_{l\inotnull} x_{lj}x_{ij}\tilde a_l
		= \suml_{j=1}^N p_jy_jx_{ij} -\frac\mu2 \tilde e_i, i\inotnull.
\end{equation*}

В матричных обозначениях система принимает вид,
\begin{equation*}
	\cbr{\tilde{\mb X}^T_\lams P\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}} \tilde{\mb a}
		= \tilde{\mb X}^T_\lams P y -\frac\mu2\tilde{\mb e}_\lams,
\end{equation*}
а решение может быть выражено в виде
\begin{equation}
	\besta
		= \inv{\tilde{\mb X}^T_\lams P\tilde{\mb X}_\lams + \beta I_{\hat n_\lams}}\cbr{\tilde{\mb X}^T_\lams P y -\frac\mu2\tilde{\mb e}_\lams}.
\end{equation}

Утверждение доказано.
\end{Proof}

