Функция Лагранжа:
\begin{equation}
	\label{LagrWEN}
	\begin{split}
		L(\mb a,\idots \delta1N, \idots \lambda1N) 
	&= \suml_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i}} + \suml_{j=1}^N p_j\delta_j^2 - \sum_{j=1}^N\lambda_j\cbr{\delta_j - y_j + \suml_{i=1}^nx_{ij}a_i} = \\
	&= \suml_{i=1}^n\cbr{\beta a_i^2 + \mu \modul{a_i} - \suml_{j=1}^N\lambda_jx_{ij}a_i} + \suml_{j=1}^N \cbr{p_j\delta_j^2  - \lambda_j\cbr{\delta_j - y_j}} \to \\
	&\to 
	\begin{cases}
		\min_{\mb a,\idots \delta1N}\\
		\max_{\idots \lambda1N}\\
	\end{cases}
	\end{split}
\end{equation}



\def\LiPart{L_i(a_i,\idots \lambda1N)}
\def\sumLambdaX{\suml_{j=1}^N\lambda_jx_{ij}}
\newcommand\diag[1]{\text{diag}\cbr{#1}}
Положим 
\begin{equation}
	\label{partialLagr}
	\LiPart = \beta a_i^2 + \mu \modul{a_i} - \cbr{\sumLambdaX}a_i \to \min_{a_i}.
\end{equation}

Каждая из функций $L_i$ является кусочно заданной комбинаций двух квадратных трехчленов:
\begin{equation}
	\label{partialLagrParted}
	\begin{split}
		\LiPart 
		&= \begin{cases}
			\beta a_i^2 + \mu a_i - \cbr{\sumLambdaX}a_i, &a_i > 0,\\
			0, &a_i = 0,\\
			\beta a_i^2 - \mu a_i - \cbr{\sumLambdaX}a_i, &a_i < 0,\\
		\end{cases} \quad= \\
		&= \begin{cases}
			\beta a_i^2 - \cbr{\sumLambdaX-\mu}a_i, &a_i > 0,\\
			0, &a_i = 0,\\
			\beta a_i^2 - \cbr{\sumLambdaX+\mu}a_i, &a_i < 0.\\
		\end{cases}	
	\end{split}
\end{equation}

Исходная постановка задачи Elastic Net \ref{basicEN} предполагает, что $\beta\ge0, \mu\ge0.$ Изучим случай $\beta > 0:$ 
\begin{equation}
	\label{hatAi}
	\begin{split}
		\hat a_i &= \argmin_{a_i} \LiPart = \\
				 &=	\argmin_{a_i} \fbr{
				 	\begin{aligned}
						\beta a_i^2 - \cbr{\sumLambdaX-\mu}a_i, &a_i > 0,\\
						0, \qquad\qquad &a_i = 0,\\
						\beta a_i^2 - \cbr{\sumLambdaX+\mu}a_i, &a_i < 0.\\
					\end{aligned}	
					}
	\end{split}
\end{equation}

Для поиска точки минимума рассмотрим 3 случая:
\begin{align}
	\label{hatAicases}
	         &\sumLambdaX \le -\mu 	&\Rightarrow \hat a_i &= \frac{\sumLambdaX+\mu}{2\beta}, \\
	-\mu <   &\sumLambdaX < \mu 	&\Rightarrow \hat a_i &= 0, \\
	-\mu \le &\sumLambdaX 			&\Rightarrow \hat a_i &= \frac{\sumLambdaX-\mu}{2\beta},
\end{align}

% a x^2 + b x + c = a(x-b/{2a})^2 + c - b^2/{4a} = a(x-x_0)^2 + c - a x_0^2

Подставляя значения из \ref{hatAicases} в \ref{partialLagrParted}, получим решение задачи минимизации по $a_i$:
\begin{align}
	\label{hatLiSquared}
	\begin{split}
		\hat L_i(\idots \lambda1N) &= \min_{a_i}L_i(a_i, \idots \lambda1N) =
		\begin{cases}
			-\cfrac1{4\beta}\cbr{\sumLambdaX+\mu}^2, &\sumLambdaX \le -\mu,\\
			0, 										&-\mu < \sumLambdaX < \mu,\\
			-\cfrac1{4\beta}\cbr{\sumLambdaX-\mu}^2, &\mu\le\sumLambdaX,
		\end{cases} = \\
		&= -\frac1{4\beta}
		\begin{cases}
			\cbr{-\sumLambdaX-\mu}^2, &\sumLambdaX \le -\mu,\\
			0, 	 					&-\mu < \sumLambdaX < \mu,\\
			\cbr{\sumLambdaX-\mu}^2, &\mu\le\sumLambdaX.
		\end{cases} 	\\
		&= -\frac1{4\beta} \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu-\sumLambdaX}}^2
	\end{split}
\end{align}
(Здесь не сходится с результатами doc-файла)

Теперь перейдём к минимизации по $\cbr{\delta_1, \ldots, \delta_N}:$
\begin{equation}
	0 = \dd{}{\delta_j}L(\mb a,\idots \delta1N, \idots \lambda1N) = 2p_j\delta_j - \lambda_j 
\end{equation}
\begin{equation}
	\label{deltaJ}
	\delta_j = \frac{\lambda_j}{2p_j}
\end{equation}

Подставим результаты (\ref{hatLiSquared}) в (\ref{LagrWEN}):
\begin{align} 
	\hat L&\cbr{\mb a,\idots \delta1N, \idots \lambda1N} 
		= \suml_{i=1}^n \hat L_i(a_i,\idots \lambda1N)
		+ \suml_{j=1}^N \cbr{p_j\delta_j^2  - \lambda_j\cbr{\delta_j - y_j}} = \\
		&= -\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu-\sumLambdaX}}^2
		+ \suml_{j=1}^N \cbr{p_j\cbr{\frac{\lambda_j}{2p_j}}^2  - \lambda_j\cbr{\frac{\lambda_j}{2p_j} - y_j}} = \\
		&= -\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu - \sumLambdaX}}^2
		- \suml_{j=1}^N \cbr{ \frac{\lambda_j^2}{4p_j}  - \lambda_j y_j} 
\end{align}

Таким образом, задача (\ref{LagrWEN}) свелась к 
\begin{align}
-\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu - \sumLambdaX}}^2
		- \suml_{j=1}^N \cbr{ \frac{\lambda_j^2}{4p_j}  - \lambda_j y_j} \to \max_{\idots \lambda1N}
\end{align}
Используя соотношение (\ref{deltaJ}), получим эквивалентную задачу:
\begin{align} 
	% \hat L&\cbr{\mb a,\idots \delta1N, \idots \lambda1N} 
	% 	= \suml_{i=1}^n \hat L_i(a_i,\idots \lambda1N)
	% 	+ \suml_{j=1}^N % \cbr{p_j\cbr{\frac{\lambda_j}{2p_j}}^2  - \lambda_j\cbr{\frac{\lambda_j}{2p_j} - y_j}} = \\
	% 		\cbr{p_j\delta_j^2  - \lambda_j\cbr{\delta_j - y_j}} = \\
	% 	&= -\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \sumLambdaX, 0, \mu-\sumLambdaX}}^2
	% 	- \suml_{j=1}^N \cbr{p_j\delta_j^2  -  2 \delta_j p_j (\delta_j - y_j)} = \\
	% 	&= 
	-\frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \suml_{j=1}^N2 \delta_j p_jx_{ij}, 0, \mu-\suml_{j=1}^N2 \delta_j p_jx_{ij}}}^2
		- \suml_{j=1}^N \cbr{ \delta_j^2 p_j  - 2\delta_j p_j y_j} \to \max_{\idots \delta1N}
\end{align}
Отметим, что задача осталась задачей максимизации.

Положим 
\begin{align}
	W(\idots \delta1N) 
	= \frac1{4\beta} \suml_{i=1}^n \cbr{\min\fbr{\mu + \suml_{j=1}^N2 \delta_j p_jx_{ij}, 0, \mu-\suml_{j=1}^N2 \delta_j p_jx_{ij}}}^2
		+ \suml_{j=1}^N \cbr{ \delta_j^2 p_j  - 2\delta_j p_j y_j}
\end{align}

Итак, достаточно решить задачу 
\begin{align}
	W(\idots \delta1N) \to \min_{\idots \delta1N},
\end{align}
или же, используя матричные обозначения $\mb \delta = \cbr{\idots \delta1N}^T \in \mathbb R^N, \mb x_i = \cbr{x_{i1}, \ldots, \mb y = \idots y1N, x_{iN}}^T \in \mathbb R^N, P = P^T= \diag{\sqrt{p_1}, \ldots, \sqrt{p_N}},$

\begin{align}
	W(\mb \delta) = \frac1{2\beta} \suml_{i=1}^n \cbr{\min\fbr{\frac\mu2 + \mb \delta^T P^2\mb x_i, 0, \frac\mu2- \mb \delta^T P^2\mb x_i}}^2
		+ (\mb \delta - \mb y)^T P (\mb \delta - \mb y) \to \min_{\mb \delta}.
\end{align}

Пусть $\hat{\mb \delta}$~--- решение этой задачи. Оно единственно в силу выпуклости функции $W(\mb \delta)$ по $\mb \delta$.