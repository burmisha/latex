\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel} %comment it for english!
\usepackage{amsfonts,longtable,amssymb,amsmath,array}
\usepackage{listings}
%\usepackage{euler}

\usepackage{tikz}

%\usepackage{euler}
\usepackage{wrapfig} %\usepackage{floatflt}
\usepackage{tikz}

\input pagestyle.tex
\input macro.tex


\begin{document}

\section{Теоретическое домашнее задание (4). \\ Михаил Бурмистров. i.like.spam@ya.ru}

\subsection{Задача 1}
Построим по словам из файла префиксное дерево, а в его вершины сохраним частоты соответствующих запросов. 
Затем обходом от листьев к корню (например, рекурсивно) каждому узлу сопоставим топ-20 наиболее частотных запросов с таким же началом (это легко определить, зная топ-20 у каждого из потомков и частоты самих потомков). 
Если потомков нет или их меньше 20, то соответствующий список топ-20 у этой вершины пуст или же неполон (в нем менее 20 элементов). 
Отметим, что эти топ-20 к каждой вершине можно хранить, например, в «куче», что способно сократить время предобработки.

Сложность. Если использовать несжатое префиксное дерево, то на его построение потребуется операций порядка суммарной длины запросов (обозначим её за $L$). 
После этого требуется произвести описанный обход дерева. Пусть у нас $N$ вершин в дереве, $K$ --- максимальная длина запроса в файле (глубина дерева, по сути дела), 
$\mathcal{A}=\fbr{a_1,\ldots,a_{A}}$ --- алфавит языка запросов ($A$ --- максимальное число потомков). 
Если хранить каждый список в куче, то сложность отбора топ-20 для каждой вершины не будет превышать $20\cdot(A\log 20 + 1 + \log 20)$ (учтено сравнение с самой вершиной и необходимость создания новой кучи), 
т.е. в результате имеем еще $NA\cdot20\log20$ операций. Итого, нам заведомо понадобится не более $\obol{LA}$ операций на предобработку.

Что касается памяти, то само дерево займет $\obol{L}$, а также по куче в каждую вершину — ещё $\obol{NK\cdot20}$ (в худшем случае). 
Имеем $\obol{L}$ --- столько же, сколько и было в файле.

Обработка запроса длины $M$ заключается в поиске по префиксному дереву соответствующей вершины и выписыванием элементов кучи: $\obol{M}$.

\subsection{Задача 2. MapReduce}
\subsubsection{Join}
Операция Map первой таблицы: $(k,v) \to (k,0v)$.\\
Операция Map второй таблицы: $(k,v) \to (k,1v)$ (дописать к рез-ту 1-го Map).\\
Reduce к полученной таблице: для всех записей с ключом $k$ отсортировать соответствующие значения (их всего два), убрать по первому символу, склеить и получить $V$, записать в результат пару $(k,V)$.
\subsubsection{Single Source Shortest Path}
Непонятно, как хранится граф. Предположим, что он хранится в MR-таблице в виде (id-вершины, список пар (вершина, расстояние до неё)). Будем называть эту таблицу $A=(k,\fbr{l,d})$  ($k,l$ --- вершины, $d$ --- расстояние)

Инициализация: $Map(A) \to B: (k,v) \to (k, k==u \:? \:0: Inf)$.

Затем: $$Join(A,B) \to C: (k,\fbr{l,d}), (k, dist) \to (k,\fbr{l,d}, dist)$$.
$$Map(C) \to D: (k,\fbr{l,d}, dist) \to \fbr{(l, d+dist)}$$.
$$Reduce(D) \to B: \fbr{(k,d)} \to (k,\min\fbr{d})$$.

Повторяем эти 3 операции, пока $B$ именяется. (Для того, чтобы это установить, третью операцию делаем в $E$, затем делаем $Join(B,E)$ и если везде совпадение --- выходим из цикла. Ну и не забыть перенести $E$ в $B$).

Задачка отлично ложится на Mesh :-)
\subsubsection{Логистическая регрессия}
Немного формализуем, чего мы хотим добиться:
\al{
    \theta &= \arg\max_\theta\Ell(\theta), \\
    \Ell(\theta) &= \sum_{i=1}^m \fbr{y_i\log f(\theta^Tx_i) + (1-y_i)\log(1-f(\theta^Tx_i))}, \\
    f(x) &= \frac1{1+e^{-z}}.
}

И будем считать, что число объектов гораздо больше числа признаков, и именно оно требует строить модель регрессии распределенно.
Применим метод градиентного спуска (чем проще метод, тем лучше — обращать матрицы при большом числе признаков даже распределенно может быть затруднительно)
\al{
    \theta^{j+1} &= \theta^j + \alpha\nabla\Ell(\theta^j) = \theta^j + \alpha\sum_{i=1}^m(y_i - f(\theta^Tx_i))x_i, \\
}

Одна итерация спуска --- одна пара Map-Reduce у нас.
Итак, пусть у нас в MR-таблице лежат признаки и ответы (ключом будет id объекта). Имеем некоторое текущее $\theta$. Map отображает $(k_i, x_i) \to (0, y_i - f(\theta^Tx_i))$. Reduce суммирует все результаты с общим ключом (а он у нас сейчас всего один). (Проблемы могут возникнуть, если MR-архитектура предполагает хранение всех записей с одни ключом на одной физической машине, но это нас сейчас не касается.) После этого клиент может самостоятельно досчитать новое значение $\theta$, поменять $\alpha$ по своему усмотрению и запустить новую итерацию.
% Применим метод Ньютона для нахождения оптимальных параметров:
% \al{
%     \theta^{j+1} &= \theta^j - \mathcal{H}(\theta^j)\nabla\Ell(\theta^j), \\
%     H&=\sbr{\frac{\partial^2 \Ell}{\partial \theta_i\partial \theta_j}}_{i=1,\ldots,n}^{j=1,\ldots,n}
% }

\subsection{Задача 3. PageRank}
\subsubsection{Конь}
Идея. Пронумеруем все клетки доски числами от 1 до 64. Всё, что снаружи доски, пронумеруем одним числом: 65. Составим матрицу вероятностей переходов: $A = \|a_{ij}\|_{i=1,\ldots,65}^{j=1,\ldots,65}$ --- вероятность перейти из клетки номер $i$ в клетку $j$. (например из центральных клеток доски мы будем попадать с вероятностью по $\frac18$ на клетки доски, из угловых --- с вероятностью $\frac34$ в 65-ю, а из 65-й с вероятностью 1 в 65-ю).

Пусть мы ищем вероятность попасть из клетки номер $k$ за $n$ шагов а пределы доски. Для этого составим строку: $p_0=\fbr{\delta_{k,65}}$ ($\delta_{ij}$ — символ Кронекера). И изучим строку $p = p_0A^n$. В ней на 65-й позиции стоит искомая вероятность.

\subsubsection{Число от 0 до 1000}
Можно действовать аналогично. Пусть имеем 2001 состояние (с номерами 0 до 2000, ведь дальше двухтысячного мы за 1000 шагов не уйдём). Точно так же составим матрицу переходов $A$: вероятности перейти в соседнее состояние по $\frac12$ (опять-таки из 0 никуда не переходим). Вновь $p_0=\fbr{\delta_{nj}}_{j=0,\ldots,2000}$ и изучаем первую (т.е. нулевую) позицию в $p = p_0A^{1000}$ — она равна искомой вероятности.

\subsection{Задача 4. Постинг лист}
Переформулируем задачу. Есть случайная неупорядоченная $m$-выборка без повторений из $N$ элементов (одна из $C_N^m$ возможных): $(l_1,\ldots,l_m)$. 

Положим $l_0 = 0, d_i = l_i - l_{i-1}, i=\ol{1,m}.$ 

Ищем $P(\exist{i\in\ol{1,m}} d_i = k) = 1 - P(\foral{i\in\ol{1,m}}\:d_i \ne k) $.

Расcмотрим все возможные $i$. Пусть $l_i-l_{i-1} = k.$ Тогда между 1 и $l_{i-1}$ должно «уместиться» еще $i-2$ число, а между $l_i$ и $N$ — ещё $m-i$. 

Т.е. $l_{i-1} \ge i-2, l_i \le N - m + i$. Имеем: $i + k - 2 \le l_i \le N-m+i$.
Число подходящих перестановок равно 
$$\sum_{i=2}^m\sum_{l=i+k-2}^{N-m+i} C_{l-k-1}^{i-2}\cdot C_{N-l}^{m-i} + C_{N-1}^{m-1}.$$

Эта оценка завышена, поскольку мы подсчитали некоторые перестановки по несколько раз, однако при $m$ заметно меньших $N$ она должна быть довольно точной.

Ответ: $\cfrac{\sum\limits_{i=2}^m\sum\limits_{l=i+k-2}^{N-m+i} C_{l-k-1}^{i-2}\cdot C_{N-l}^{m-i} + C_{N-1}^{m-1}}{C_N^m}$
% Рассмотрим одну из таких случайных выборок. Её можно сгенерировать, например, так: выбрать 

\subsection{Задача 5. Сегментация запросов}
У нас имеются словосочетания — запросы. Надо каждое из них разбить на части, которые были бы самостоятельными часто встречающимися запросами, т.е. сегментировать. Поставим перед собой задачу найти такую сегментацию, что произведение соответствующих вероятностей максимально.

Будем действовать рекурсивно, поскольку каждая непрерывная последовательность сегментов — сама по себе наилучшая сегментация соответствующего участка запроса. (Либо же можно переписать это в терминах динамического программирования.)

Отметим, что «жадное» наращивание сегментов не сработает при такой постановке задачи.

\subsection{Задача 6. Словарь}
Сперва закодируем все слова и запросы id-шниками (что, впрочем, не изменит интересующую нас асимптотику). Построим два индекса. Первый: (запрос, слова в запросе); второй: (слово, запросы с таким словом). 

Когда нам приходит запрос из $m$ слов, делаем следующее: для каждого из $m$ слов запроса берем по списку запросов, в которых они встречаются. Среди них нас интересуют ровно те, которые содержат все слова исходного запроса, потому выберем именно их (например, можно слить эти списки id-шников запросов в новый список, отсортировать его и выбрать лишь те элементы, которые встретились $m$ раз, но кажется, что можно эффективнее). Оставшиеся запросы теперь следует отсортировать по числу слов в них и отдать топ-10 (чем меньше, тем лучше).

Оценка сложности. Память: индексы займут $\obol{\text{суммарная длина запросов}}$.

Вычислительная сложность в самом плохом случае: $Sort(mN)*2$, т.е. $\obol{mN\log(mN)}.$ На практике для этого должен прийти запрос, содержащий все слова в словаре.
\end{document} 