\documentclass[unicode,lefteqn,c,hyperref={pdfpagelabels=true}]{beamer}
\usepackage{burslides}
\include{macro}

\title[\hbox to 56mm{MDL principle \hfill\insertframenumber\,/\,\inserttotalframenumber}]{Принцип минимальной длины описания \\ Minimum description length principle}
\author[Михаил Бурмистров]{Михаил Бурмистров}
\institute{Основано на работе 
		Peter Gr\"unwald
		\vfill \textit{A Tutorial Introduction to 
		\vfill the Minimum Description Length Principle} 
		\vfill Centrum voor Wiskunde en Informatica, 
		\vfill The Netherlands, 2004.}
\date{\today}

\begin{document}

\begin{frame}
    \maketitle
\end{frame}

\begin{frame}
	\begin{block}{Основная задача}
	Построить критерий сравнения моделей обучения.
	\end{block}
	\begin{block}{Основная идея}
	Любая закономерность в данных может быть использована для того, чтобы их сжать. Поиск закономерностей и регулярностей в данных в подходе отождестваляется с обучением.
	\end{block}
	\textbf{Сжатие данных}~--- описание данных меньшим количеством символов, чем потребовалось бы для из записи обычным способом.
\end{frame}

\begin{frame}
	\textbf{Особенности подхода:}
	\begin{enumerate}
		\item выбор простейшей модели, по аналогии с бритвой Оккама,
		\item отсутствие переобучения, 
		\item наличие Байесовской интерпретации,
		\item отсутствие необходимости в априорных предположениях,
		\item построенная/выбранная модель обладает предсказательной силой.
	\end{enumerate}
\end{frame}

\begin{frame}{Примеры}
	\textbf{Реальные неслучайные данные, допускающие сжатие:}
	\begin{enumerate}
		\item Последовательность цифр в записи $\pi$.
		\item Зависимость времени свободного падения шара от высоты, с которой его отпустили.
		\item Естественный язык, поскольку не любая последовательность слов синтаксически корректна.
	\end{enumerate}

	\textbf{Последовательности из $0$ и $1$:}
	\begin{enumerate}
		\item \texttt{00010001000100010001\ldots 00010001000100010001}
		\item \texttt{01110100110100100110\ldots 10111011000101100010}
		\item \texttt{00011000001010100000\ldots 00010000001000110000}
	\end{enumerate}
\end{frame}

\begin{frame}{Идеализированный подход}
	\begin{block}{Идеализированный MDL}
	Использование в качестве критерия сложности модели данных Колмогоровскую сложность её описания.
	\end{block}

	\textbf{Проблемы:}
	\begin{enumerate}
		\item невычислимость,
		\item зависимость результата от выбранной универсальной машины Тьюринга.
	\end{enumerate}
\end{frame}

\begin{frame}{Практический подход}
	\begin{block}{Практический MDL}
	Ограничиться при изучении моделей лишь частью из всех возможных. 
	При этом язык описания моделей должен быть достаточно богат, чтобы сжимать <<интуитивно>> регулярные последовательности.
	\end{block}

	\textbf{Преимущества:}
	\begin{enumerate}
		\item вычислимость.
	\end{enumerate}
\end{frame}

\newcommand\mch[1]{\mathcal H^{\cbr{#1}}}
\begin{frame}{Черновой вариант MDL}
	\textbf{Терминология:}
	\begin{enumerate}
		\item Точечная гипотеза~--- конкретная функция, алгоритм, распределение. 
		Например, полином $4x^2+0{,}119x+93.$
		\item Модель~--- совокупность точечных гипотез. Например, все полиномы степени 3.
	\end{enumerate}
	\begin{block}{Практический MDL}
	Пусть $\mch 1, \mch 2, \ldots$~--- изучаемые модели данных. Лучшей точечной гипотезой $H \in \mch 1 \cup \mch 2 \cup \ldots$ для данных $D$ назовём 
	$H = \argmin \cbr{L(H) + L(D|H)},$
	где 
	\begin{itemize}
		\item $L(H)$~--- длина описания гипотезы,
		\item $L(D|H)$~--- длина описания данных при известной гипотезе.
	\end{itemize}
	Лучшая модель~--- $\hat {\mathcal H} = \mch i: H \in \mch i.$
	\end{block}
\end{frame}

\begin{frame}{Уточненный MDL}
	\textbf{Требуемые уточнения:}
	\begin{itemize}
		\item $L(D|H)$~--- отклонение данных от гипотезы. Например, если $Y=H(X) + Z, Z \sim \mathcal N(0,\sigma^2),$ то алгоритм кодирования Шеннона-Фано даёт $L(D|H)=-\log P(D|H).$ 
		\item $L(H)$~--- всё также зависит от способа кодирования,
		\begin{itemize}
			\item минимаксный подход (наикратчайший код для описания самых плохих входных данных) ведёт к необходимости сложной дискретизации пространства моделей $\mathcal{H}$
		\end{itemize}
	\end{itemize}

	\textbf{Refined MDL:} будем кодировать сразу все данные целиком, без разбиения на модель и поправку. Получим $\bar L(D|\mathcal H),$ т.н. стохастическую сложность данных по отношению к модели.

	\begin{block}{Параметрическая сложность}
	Пусть $\hat H$~--- наиболее правдоподобная модель из семейства $\mathcal{H}$ для D.
	Тогдв $$\bar L(D|H) = L(D|\hat H) + \textbf{COMP}(\mathcal H).$$
	\end{block}
\end{frame}

\begin{frame}{Формальные требования к описанию данных}
	% Пусть задан набор объектов $x^n = \cbr{x_1,\ldots,x_n}, x_i \in \mathcal X,$ 
	% где $\mathcal X$~--- конечное или счетное множество.
	Пусть $\mathcal X$~--- конечное или счетное множество.
	
	$C \colon \mathcal X \to \bigcup_{n\ge1}\fbr{0,1}^n$~--- \textbf{код}, если $C$~--- инъективно.

	Код $C$ порождает функцию длины кодирования: $L_C \colon \mathcal{X} \to \mathbb N.$ 
	
	\begin{block}{$C$~--- префиксный код,} 
	если $\foral{x_1 \ne x_2 \in \mathcal X} C(x_1)\mbox{ не продолжает } C(x_2).$
	\end{block}

	Префиксный код  $C$ --- полный, если не существует другого префиксного кода $C'$ такого, что $\foral{x\in \mathcal{X}} L_{C'}(x) \le L_C(x),$ причем неравенство было бы строгим на некотором $x.$
\end{frame}

\begin{frame}
	\begin{block}{Связь префиксных кодов и вероятностных распределений}
	Пусть над $\mathcal X$ задано распределение $P,$ тогда существует префиксный код $C \colon \foral{x\in \mathcal X} L_C(x) = \lceil-\log P(x)\rceil.$
	
	Обратно,  для любого префиксного кода $C'$ для $\mathcal X$ найдется вероятностное распределение (возможно, дефектное) $P'$ над $\mathcal{X}$ такое, что $\foral{x\in \mathcal X} L_C'(x) = -\log P'(x).$

	Причем, $C'$ --- полный префиксный код $\Leftrightarrow$ $\sum_{x\in \mathcal X} P(x) = 1.$
	\end{block}

	% $L(D|H).$ Пусть задана выборка размера $n,$ тогда каждую гипотезу $H$ можно рассматривать как распределение вероятностей над $\mathcal{X}^n$
	% \begin{itemize}
	% 	\item минимаксный подход (наикратчайший код для описания самых плохих входных данных) ведёт к необходимости сложной дискретизации пространства моделей $\mathcal{H}$
	% \end{itemize}
\end{frame}

\begin{frame}{Предсказание поведения генов}
Ioan Tabus and Jaakko Astola, \textit{On the Use of MDL Principle in Gene Expression Prediction}

	Исходные данные: для 12 генов 30 раз измерено их состояние: $\fbr{-1,0,1}$ (угнетенное, нормальное, возбужденное). 

	Задача: предсказать их состояние в следующий момент времени.

	Рассматриваются Марковские модели с различной шириной окна $n$. Для кодирования такой модели требуется $2\cdot3^n$ бит.
	Для кодирования данных с использованием гипотезы потребуется $\lceil \log30\rceil \times \log2 \times N_e,$ где $N_e$ --- число ошибок предсказания модели.
\end{frame}

\begin{frame}{Бинарное разложение матрицы}
	Pauli Miettinen Jilles Vreeken, \textit{Minimum Description Length for Boolean Matrix Factorization}

	Дано: $A$~--- бинарная матрица $n\times m$, $k \in \mathbb N.$

	Задача: найти бинарные матрицы $B$ и $C$ размера $n\times k$ и $k\times m$ такие, что 
	$$(B,C) = \argmin\sum_{i,j}\modul{a_{ij}-\cbr{BC}_{ij}}$$

	NP-полная задача.

	Выбор $k$ осуществляется через MDL. Описанием гипотезы и данных является: 2 бинарных матрицы, матрица исправлений. При этом сложность зависит от кодирования

	Оказывается, что здесь на результат существенно влияет способ кодирования матрицы исправлений. Наилучший способ из рассмотренных: 
	разбить матрицу ошибок на 2, вычислить в каждой долю единиц, закодировать её, и использовать оптимальный префиксный код для указанной частоты единиц.
\end{frame}

\begin{frame}{}
	 \begin{center}
	 {\LARGE Спасибо за внимание.}
	 \end{center}
\end{frame}

\end{document}
